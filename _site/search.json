[
  
    {
      "title"       : "数据结构知识点整理",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86.html",
      "date"        : "2025-01-06 00:00:00 +0800",
      "description" : "",
      "content"     : "数据结构知识点整理整理人：ld日期： 2024年12月9日起（for 2024 fall）2025.1.5反馈：已完成，如有错误请各位提出感谢各位对于本人数据结构整理内容的支持和认可，祝各位考试顺利之后将考虑将pdf版上传交大传承内容挺多的，制作不易！==反馈、交流板块：==请点击这里哦:smiley::tada::tada::tada:！！！第零部分 前言目录0. 前言（我写的）：:::spoiler 我的一些个人评价，点击能打开来 作为一门只有3学分的课程，个人认为在学完了c++后，数据结构前面的部分对于有编程基础的同学（例如以应试教育方式全部学过Python以及栈、队列等概念的浙江同学）来说，还是太过于熟悉。就算是对于只是刚刚学过程设的其他工科同学来说，其难度也并不高。所以，本人说实话，在听了几节老师的课后，后面就全部没来上了:smiley:。不过必须要说一句，后面的部分确实是有难度的。 说这么多，只是想强调一下，只要程设有认真学了（尤其是class结构，以及template模板的应用），这门课在代码上基本上问题不大，相信各位，也祝各位期末加油鸭:smile::tada: 本次整理将会全部按照课本的顺序来，重点在于基础的掌握，代码一并经过解释放在最后，由于时间限制，所以做不到事无巨细，还请各位理解 目前主要参照：上课的ppt 另外，最近正好发现了这个协作笔记的网站，个人觉得确实不错，所以就在这里整理所有知识点:wink:::: 一些注意点：加粗部分是标题、概念和我认为重要的一些东西==高亮部分是一些注释，以及我对相关知识点的一些评价和带有个人主观观点的题外话== 一些以这样小框形式呈现的内容，是我给自己留的注释，用于帮助完善笔记 一些需要分类的东西会使用思维导图来展示一些重要的东西会选取范磊老师ppt上面的图片进行解释（所以可能会有问题）一些我认为需要理解的代码会放在正文里，至于其他的，可以自己看书目录：（点击可传送）==一些更低级别的标题被提到了更高一级，主要是因为个人觉得这些内容偏重点+比较有难度，从而方便一次性链接直达====取消了源代码部分，第一是我写了大家也不一定会看，第二是太过于复杂了:cry:==[TOC maxlevel=2]1.数据结构的引言 这一章重点讲解数据和算法结构，主要是基础内容 1.1数据的逻辑结构 逻辑结构与数据元素的内容、个数无关```markmap 逻辑结构 线性结构 数据元素构成有序序列 我们会在第一部分学到 树状结构 数据元素形成一个层次关系（只有一个前驱） 我们会在第二部分学到 集合结构 数据元素间的次序任意 我们会在第三部分学到 图状结构 最一般，前驱后继数目不限 我们会在第四部分学到``` 数据结构中会有各种操作和运算常见的：创建、清除、插入、删除、搜索、更新、访问、遍历（每个元素恰好被访问一次）1.2存储实现 包括两个部分： 数据元素的存储 数据元素之间的关系的存储 （实际编程中还会增加一些哑节点，比如链表中的空的头结点）```markmap 存储实现 顺序存储 连续存储区域 链接存储 存储可以分散，关系通过指针指出 哈希存储方式 专用于集合结构的数据存放方式各个结点均匀地分布在一块连续的存储区域中用一个哈希函数将数据元素和存储位置关联起来 索引存储方式 所有的存储结点按照生成的次序连续存放另外设置一个索引区域表示结点之间的关系```1.3算法分析 数据结构是讨论一组数据的处理问题，每一种操作的实现都是一个算法每种操作有多种实现方式，因此也有好坏之分，需要进行评价 数据结构主要考虑的是代码的时空性能，即时间和空间的消耗 时间复杂度分析 渐进表示法：只考虑运行时间函数的数量级 我们主要需要掌握大O表示法，只需要给出一个数量级，表示其受哪一数量级影响最大（即最大的数量级），忽略小的数量级 常见的大小比较： O(1) &lt; O(logN) &lt; O(N) &lt; O(NlogN) &lt; O(N^2^) &lt; O(N^k^)(k&gt;2) &lt; O(k^N^)(k&gt;1) &lt; O(N!) &lt; O(N^N^) 求和定理：两个程序若先后运行，运行时间受数量级高的影响 求积定理：若相乘，则时间复杂度相乘 看程序时，基本上就看代码会重复运行多少次，在程序中找出最复杂、运行时间最长的程序段，计算它的时间复杂度。也就是整个程序的时间复杂度==也就是取最大的==空间复杂度分析 直接看需要多少存储空间即可，表示方法与时间复杂度（大O）相同 根据以上的知识，我们可以通过改变算法来尝试降低其两个复杂度1.4 面向对象方法 将数据结构的存储和处理过程封装起来做成一个个工具，通过实用的工具来解决问题==也是本课会用到的方法，但是着实有点复杂==第一部分 线性结构第一部分是线性表，包括栈和队列两种有特点的线性表==个人认为难度不大，对有基础的同学们算是复习（如果觉得难的话，请忽略上一句:cry:）==2.线性表2.0定义 定义：N个具有相同特征的结点A~0~, A~1~, …, A~N-1~构成的集合。除了A~0~和A~N-1~外，每个元素都有唯一的前趋和后继 表的术语：N（表的大小）、A~0~（首结点）、A~N-1~（尾结点）、空表、A~i~在表中的位置为i线性表的实现（重点介绍其结构、算法特点）==这里必须说明，下文的代码会是能运行情况下尽量简洁的，去除了一些细节==2.1顺序实现顺序存储的优点在于其访问十分快速（根据索引找到内存地址），但是一旦要对数据进行修改，就要对很多数据进行移动，而且有可能浪费空间。2.1.1构造// 构造函数template &lt;class elemType&gt;seqList&lt;elemType&gt;::seqList(int initSize) // 传入自己确定的表的大小{ data = new elemType[initSize]; // 申请数据存储空间 maxSize = initSize; // 定义最大大小 currentLength = 0; // 初始的长度当然是0}2.1.2插入insert()与删除remove() 插入元素需要后面元素后移以留出空间，时间复杂度O(N) // 插入函数template &lt;class elemType&gt;void seqList&lt;elemType&gt;::insert(int i, const elemType &amp;x){ for (int j = currentLength; j &gt; i; j--) data[j] = data[j - 1]; // i后面的全部后移一位，空出位置来插入新元素 data[i] = x; ++currentLength; // 现有大小+1} ==注意a[currentLength]因为a的索引从0开始，所以其实是空值== 删除同理，需要后面元素移动填补空缺，时间复杂度O(N) template &lt;class elemType&gt;void seqList&lt;elemType&gt;::remove(int i){ if (i &lt; 0 || i &gt; currentLength - 1) throw OutOfBound(); // 检查删除的位置是否合法，不必多在意 for (int j = i; j &lt; currentLength - 1; j++) data[j] = data[j + 1]; // i后面的元素全部前移 --currentLength; // 现有大小-1} 2.1.3扩容doublespace() 顺序实现需要再表满的时候进行扩容（链表实现不需要），从而存储更多的数据==之前上个学期程设期末的最后一题其实就可以用这个方法实现，但是那时的我还是太菜了:cry:== // 扩大空间的函数，简洁不下去了，就有这么多行template &lt;class elemType&gt;void seqList&lt;elemType&gt;::doubleSpace(){ elemType *tmp = data; // 设立一个tmp指针，来存储原来表的数据 maxSize *= 2; data = new elemType[maxSize]; // data变为了新的、更大的表 for (int i = 0; i &lt; currentLength; ++i) data[i] = tmp[i]; // 将原来的数据copy过去 delete[] tmp; // 记得删除，避免内存泄漏} 2.2链表实现==链表的优点就是修改迅速方便，但是查找就需要慢慢地一个一个查==2.2.1构造创建一个头结点（节点中没有元素）即可 头结点的用途：避免特殊情况（如空链表等）需要额外代码处理 2.2.2插入insert()与删除remove() 操作本身的复杂度是O(1)，但是需要遍历、搜索需要插入的位置的话，复杂度就是O(N) 分为2步：1.移到要插入的地方（位置i）2.创立新节点，其指针指向a~i+1~，a~i~则指向该节点 // 插入函数template &lt;class elemType&gt;void sLinkList&lt;elemType&gt;::insert(int i, const elemType &amp;x){ node *pos; pos = move(i - 1); //第一步 pos-&gt;next = new node(x, pos-&gt;next); // 插入一个新的节点 ++currentLength;} 分为3步： 移到要删除的地方（位置i-1） 修改指针，a~i-1~的指针指向a~i+1~，删掉a~i~ 把节点删除掉释放内存 // 删除函数template &lt;class elemType&gt;void sLinkList&lt;elemType&gt;::remove(int i){ node *pos, *delp; pos = move(i - 1); delp = pos-&gt;next; // 找到想要删除的节点 pos-&gt;next = delp-&gt;next; // 绕过delp delete delp; --currentLength;} 2.2.3双链表 特点：多了一个prior字段，其指向前驱结点，另外多一个尾节点tail，其他的差别不大不过要清楚插入和删除节点相比单链表的变化 2.2.4单循环链表特点：最后一个节点指向第一个结点，一般不设头结点（了解特点即可）2.2.5双循环链表 特点：头结点中prior字段给出尾结点的地址，尾结点中next字段给出头结点的地址 一般也不设头尾结点（了解特点即可）3.栈3.0基础特点：先进后出结构，最先到达栈的结点将最晚被删除 相关概念栈底(bottom)：结构的首部（先进栈的节点的位置）栈顶(top)：结构的尾部（后进栈的节点的位置）出栈(pop)：结点从栈顶删除 进栈(push)：结点在栈顶位置插入 空栈：栈中结点个数为零时栈的实现3.1顺序实现顺序实现中除了扩容需要O(N)（平均下来N次进栈才会有一次扩容，其实整体来说复杂度也算是O(1)），其余的均为O(1)3.1.1构造用数组的后端表示栈顶，这样进栈和出栈不会引起顺序表中大量数据的移动==注意：top设为-1====栈的top指向的永远是栈顶元素的位置（所以空栈时top=-1）（与队列区分）==3.1.2进栈&amp;出栈 分成2步：1.top值+1（进）或-1（出）2.加入新值或返回出栈值 3.1.3判断空栈 判断top是否为-1即可 3.1.4扩容 顺序存储的扩容操作已经是一个绕不开的话题，这里的话扩容操作与其他的类似 3.2链接实现 3个注意点：1.不需要头结点，只需要单链表（因为栈的操作只与栈顶有关）2.单链表的头指针指向栈顶3.所有操作时间复杂度均为O(1) 3.2.1构造 将top设为空指针（NULL）==就是这么简单== 3.2.2插入 1.创立新元素，其指向原有的栈顶元素2.头指针指向新元素==头指针指向栈顶== 3.2.3删除 1.指针指向下一个元素2.返回出栈元素 3.2.4判断空栈 只要判断top指针是否为空即可 3.3栈的特点 1.递归函数的调用结构类似于栈，如快速排序等等==所以如果想让使用递归函数的算法不用递归函数，可以自己人工构造一个栈来模拟函数的调用过程==2.符号匹配（如括号）与检查可以使用栈来实现 ==关于栈最小空间大小的讨论：（这只是个人的经验，听不懂也没关系）1.如果是根据出栈的序列判断栈的大小（即从理论上），那么如果一个元素入栈后马上出栈，那么也会占据一个空间2.如果是例如后缀式实现的分析，那么在实际处理中，一个元素岀栈马上入栈，可以不占据空间==3.4表达式的计算（栈的应用）其用栈来实现，不过内容比较重要对于一个表达式 a+b前缀式：+ab中缀式：a+b后缀式：ab+ 后缀式可以不考虑运算符的优先级（不使用括号）3.4.1如何用栈实现后缀表达式1.若读到的是操作数，则将其进栈。2.若读到的是运算符，则将栈顶的两个操作数出栈，（（除法时）后弹出的操作数为被操作数，先弹出的为操作数）使其完成运算符所规定的运算，并将结果进栈。（1个符号对应2个数）3.最后栈里面剩下的数就是结果。3.4.2如何用栈实现中缀表达式转后缀表达式（式子里的数字直接输出）| 符号 | 操作 || – | – ||(|直接进栈||)|将栈中的运算符依次出栈。出栈操作一直进行到遇到相应的开括号为止。将开括号出栈。|| + - * / |如果栈顶运算符优先级高，则栈顶运算符出栈；出栈操作一直要进行到栈顶运算符优先级比它低为止，然后将新读入的运算符进栈保存。||结束| 将栈中所有的剩余运算符依次出栈，并放在操作数序列之后，直至栈空为止。|一个例子：4.队列4.0基础特点：先进先出结构，==难道不就跟排队一样？进去肯定先是队尾，然后排队到队头==队列图示&amp;相关概念：队列的实现4.1顺序实现使用数组存储队列中的元素队列中的结点个数最多为MaxSize个 元素下标的范围从0到MaxSize-1。 组织方式： 1.队头位置固定 缺点：出队会让大量数据移动，O(N)复杂度，==所以还是别这样实现吧== 2.队头位置不固定 判断队满：rear = MaxSize - 1（实际上元素个数不一定是Maxsize）缺点：O(1)复杂度，浪费空间，==我更讨厌这个，不可持续发展！:anger:====注意：front指向的是队首，这个地方没有元素，rear指向队尾，这里有元素== 3.循环队列（队头位置也不固定） 两个指针到了最后都回到第一个（即下标0，这样可以重复利用空间）只不过有效存储单元少一个（区别空队列（rear=front）和满队列（(rear+1)%maxsize=front)）（注意+1） //入队操作：rear = (rear + 1) % MaxSize;elem[rear] = x;//出队操作：front = (front + 1) % MaxSize;//队列为空判断if(front == rear)//队列为满判断if((rear + 1) % MaxSize == front) //即队尾往后移一个就是队首 doublespace()扩容函数 （这里有变化了） template &lt;class elemType&gt;void seqQueue&lt;elemType&gt;::doubleSpace(){ elemType *tmp = elem; elem = new elemType[2 * maxSize]; for (int i = 1; i &lt; maxSize; ++i) elem[i] = tmp[(front + i) % maxSize]; //复制原有的信息到扩容的地方，但是根据循环列表的性质，是(front + i) % maxSize front = 0; rear = maxSize - 1; maxSize *= 2; delete tmp;} 4.2链接实现 ==（我觉得队列用链接实现更好一些）== 特点：1.没有头结点的设立2.我们把单链表的表头作为队头，单链表的表尾作为队尾。队列要对表的两端作操作，所以我们同时记住头尾结点的位置，需要两个指向单链表结点的指针front和rear3.队列为空or初始化（构造）时，单链表中没有结点存在，即头尾指针都为空指针 4.3队列的应用 其实就是模拟各种需要“排队”的场景 ==第5章应该没学==12.12完成第一部分第二部分 树状结构第二部分是树，包括二叉树和堆。==对我个人而言，树的最最基础的理论知识有些学过，按照高考标准学的，所以掌握程度自行体会（doge）。但是有些东西（如AVL树）的确是没学过的，这些知识个人觉得算是数据结构里最难的（尤其是AVL树），大家共勉。==6.树6.0基础树形结构是处理具有层次关系的数据元素(1)有一个被称之为根(root)的结点(2)其余的结点可分为m(m≥0)个互不相交的集合T~1~，T~2~，…，T~m~，这些集合本身也是一棵树，并称它们为根结点的子树(Subtree)。每棵子树同样有自己的根结点。(3)可以有多个直接后继，只能有一个直接前驱==从图形上长得像棵树不就行了==6.1相关术语==必须要提的一句是，“结点”和“节点”两个写法似乎都是对的，我后面打字可能两种写法都会爆出来==# 术语## 结点### 根结点- 起始节点，没有父节点### 叶结点- 没有**子节点**（度为0）### 内部节点- 上两个类型的补集### 儿子结点- 结点的**下一层**结点### 父亲结点- 结点的**上一层**结点### 兄弟结点- 同一**父节点**的结点### 祖先结点- 从**根到该节点**的所有结点### 子孙（后代）结点- 某个节点的**所有下层**结点## 其他### 度- 直接子结点的数量- 树的度由**度最大**的结点决定### 层次- 直观地看在第几层即可- 根结点在**第1层**！！！### 高度- 看有最大几层即可### 有序树- 子树从左到右有顺序，即分左右子树### 无序树- 子树无顺序### 森林- 若干树的集合，树互不相交6.2二叉树6.2.1概念==我觉得还是简单地去理解：分成两叉的树（注意是个有序树，区别左右）==满二叉树：高度为k并具有2^k^-1个结点的二叉树（每一层结点个数都达到了最大值）完全二叉树：==我觉得满二叉树+最下面一层结点依次从左至右更好理解一点==6.2.2性质# 性质## 1.第i层上最多有2^(i-1)个结点（i≥1）（废话）## 2.高度为k的二叉树，最多具有2^k-1个结点（2的等比数列求和）## 3.如果叶子结点数为n0，度数为2的结点数为n2，则有:n0=n2+1成立。## 4.具有n个结点的完全二叉树的高度 k = log2n（向下取整）+1 ## 5.一个完全二叉树若树的根节点编号1每一层自左至右依次编号，一个节点编号为i## 接5.父节点编号为i/2（向下取整）## 接5.左儿子的编号为2i，右儿子编号2i+1（如果有的话，即&lt;n）6.2.3遍历由于树的特殊结构，所以有不同的遍历方式 前序遍历：根左右==形象地说，这种遍历能往左走就往左走，不行的时候往右遍历==分析时第一个一定是根节点。 中序遍历：左根右分析时，关注根节点的位置（会在遍历序列的中间），在其左边的就是左子树，在右边的就是右子树。 后序遍历：左右根分析时，最后一个一定是根节点。 这三种遍历方式采取的是递归的思想，即遍历到子树之后，将子树视为一个树继续遍历。只要知道一棵树的前、中or中、后遍历（前、后不行，有时候无法辨别左右子树关系）的具体情况，我们就能推知树的所有结构信息。 层次遍历：一层一层从左至右遍历过来（最废话的一种方式）二叉树实现6.2.4顺序存储根据左子树2i，右子树2i+1的特点即可。根结点的下标为1。可能会浪费存储空间。6.2.5链接存储（更为通用） 根据存不存父节点，有两种版本 标准形式（只存左右节点）： | left | data | right || —- | —- | —– | 广义标准形式（还存父节点）：从左至右:| data | left | parent | right || —- | —- | —— | —– |构造设根节点指针root = NULL（初始为空指针）isempty()判断root是否为NULL前中后递归遍历采取递归的形式，根据是前序、中序还是后续来安排遍历顺序。基本来说前序遍历在需要遍历的操作中选择的次数会多一点，我觉得因为前序遍历更符合我们遍历的直觉。//这个是前序遍历，需要注释template&lt;class T&gt;void binaryTree&lt;T&gt;::preOrder(binaryTree&lt;T&gt;::Node *t) const{ if (t == NULL) return; cout &lt;&lt; t-&gt;data &lt;&lt; ' ';//先输出根结点 preOrder(t-&gt;left);//第二个是左结点 preOrder(t-&gt;right);}6.2.5前中后非递归遍历（不知道是不是重点） 前序遍历：开始时，把二叉树的根结点存入栈中。然后重复以下过程，直到栈为空：从栈中取出一个结点，输出根结点的值，然后把右子树，左子树放入栈中。 中序遍历：根节点进栈后，先让左子树进栈。左子树访问完后自然就到根节点，访问完后右子树进栈。所以，根结点要进栈两次。当要遍历一棵树时，将根结点进栈。根结点第一次出栈时，它不能被访问，必须重新进栈，并将左子树也进栈。根节点第二次出栈是正常的 template &lt;class Type&gt;void BinaryTree&lt;Type&gt;::midOrder() const{ linkStack&lt;StNode&gt; s; StNode current(root); cout &lt;&lt; \"中序遍历: \"; s.push(current); while (!s.isEmpty()) { current = s.pop(); if (++current.TimesPop == 2) // 意思就是访问过一次（左子树已经遍历），访问一次就输出 { cout &lt;&lt; current.node-&gt;data; if (current.node-&gt;right != NULL) s.push(StNode(current.node-&gt;right)); } else // 还没访问左子树，那么先访问左子树 { s.push(current); // 重新入栈！ if (current.node-&gt;left != NULL) s.push(StNode(current.node-&gt;left)); } }} 后序遍历：根节点需要出栈三次，其他的倒没什么（代码略）6.2.6层次遍历层次遍历得专门讲一讲，这里少见的不用递归函数，而是利用了队列，在遍历时根结点出队时，会让其子结点入队，从而保证了层次遍历的有序性。（不理解的话自己可以模拟一下） 创建树也利用到了层次遍历的相关用法，即遍历到的时候输入其两个子结点的值，然后两个子结点入栈，准备之后的再一轮输入。正文部分就不再重复代码了。 输出树也是同理的，利用层次遍历进行输出，这样的遍历方式更加易于我们观看和了解。 template &lt;class T&gt;void binaryTree&lt;T&gt;::levelOrder() const{ linkQueue&lt;Node *&gt; que; Node *tmp; cout &lt;&lt; \"\\n层次遍历：\"; que.enQueue(root); while (!que.isEmpty()) { tmp = que.deQueue(); // 队头出队 cout &lt;&lt; tmp-&gt;data &lt;&lt; ' '; // 访问队头结点 if (tmp-&gt;left) que.enQueue(tmp-&gt;left); // 左子树入队 if (tmp-&gt;right) que.enQueue(tmp-&gt;right); // 右子树入队 }} 6.3哈夫曼树6.3.0编码用树来表示 计算机每个字符用一个编码表示，而且是二进制的 对应在树中的话，0相当于向左，1相当于向右 6.3.1特点 目的：根据数据的出现频率进行不同长度的编码，从而节省空间 哈夫曼树是一棵最小代价的二叉树，在这棵树上，所有的字符都包含在叶结点上。要使得整棵树的代价最小，显然权值大的叶子应当尽量靠近树根，权值小的叶子可以适当离树根远一些 6.3.2构建方法 从当前集合中选取并去除权值最小、次最小的两个结点，以这两个结点作为内部结点 b~i~ 的左右儿子，b~i~ 的权值为其左右儿子权值之和（生成了一棵新树）并加入其中。这样，在集合A中，结点个数便减少了一个。这样，在经过了n-1次循环之后，集合A中只剩下了一个结点，这个结点就是根结点。 例子：（省略中间过程）==哈夫曼编码可以保证通过连续的一串编码得出相关内容==6.3.3实现注意点：新生成的结点编号从size-1到1（即n-1个），原有的结点位置从size到2×size-1这里就放一下构造新二叉树的过程for (i = size - 1; i &gt; 0; --i){ min1 = min2 = MAX_INT;//初始化最小值 x = y = 0; for (int j = i + 1; j &lt; length; ++j) if (elem[j].parent == 0)//找到最小值和次小值 if (elem[j].weight &lt; min1) { min2 = min1; min1 = elem[j].weight; x = y; y = j; } else if (elem[j].weight &lt; min2)//如果不是最小值，则看看是不是次小值 { min2 = elem[j].weight; x = j; } elem[i].weight = min1 + min2;//更新新的节点权值 elem[i].left = x; elem[i].right = y; elem[i].parent = 0; elem[x].parent = i; elem[y].parent = i;}6.4树和森林6.4.1树的存储实现 标准形式（广义的树需要加个指向父亲节点的指针） 孩子链表示法将每个结点的所有孩子组织成一个链表。树的节点由两部分组成：存储数据元素值的数据部分、指向孩子链的指针 孩子兄弟链表示法 双亲表示法适合求祖先结点，但不便求子孙结点 6.4.2树的遍历 前后序遍历：没有中序遍历，二叉树的左右结点变为一个一个子结点遍历，==我觉得废话== 层次遍历：==废话== 6.4.3树、森林和二叉树 二叉树是结构最简单、运算最简便的树形结构，但对很多问题来讲，其自然的描述形态是树或森林。树的孩子兄弟链表示法（见上面）就是将一棵树表示成二叉树的形态。 森林 7.优先级队列7.0定义 结点之间的关系是由结点的优先级决定的，而不是由入队的先后次序决定。优先级高的先出队，优先级低的后出队。这样的队列就是优先级队列 7.1简单实现（利用已学的东西） 两种方法： 入队时，按照优先级在队列中寻找合适的位置（O(N)），将新入队的元素插入在此位置。出队操作的实现保持不变（O(1)）。 入队操作的实现保持不变，将新入队的元素放在队列尾（O(1)）。但出队时，在整个队列中查找优先级最高的元素（O(N)），让它出队。 7.2二叉堆堆是一棵完全二叉树（结构性），而且具有（有序性）（见下）==相同的数据可以有不同的建堆结果== k~i~≤k~2i~且 k~i~≤k~2i+1~(i=1,2,…,[n/2])最小化堆 k~i~≥k~2i~且 k~i~≥k~2i+1~(i=1,2,…,[n/2])最大化堆 二叉堆的实现（以最小化堆为例） 可以顺序存储 插入enqueue 新结点会先插入到完全二叉树最后一个结点，然后根据情况来向上移动 新结点的向上移动称为向上过滤(percolate up)（时间复杂度O(logN)） template &lt;class Type&gt;void priorityQueue&lt;Type&gt;::enQueue(const Type &amp;x){ // 向上过滤 int hole = ++currentSize; for (; hole &gt; 1 &amp;&amp; x &lt; array[hole / 2]; hole /= 2) // 只要数据够小，就会一直往下 array[hole] = array[hole / 2]; // hole/2是父节点，hole是子节点，父节点数据向下移动 array[hole] = x;} 删除dequeue 找到空结点（实际上是使用最后一个结点替换了该结点）的一个较小的子结点放入空结点，移动空结点往下一层 空结点是往下移动（向下过滤，即percolateDown，后面建堆会用到）（时间复杂度O(logN)） //需要追加解释template &lt;class Type&gt;Type priorityQueue&lt;Type&gt;::deQueue(){ Type minItem; minItem = array[1]; array[1] = array[currentSize--]; percolateDown(1);//从根节点开始向下调整 return minItem;}template &lt;class Type&gt;void priorityQueue&lt;Type&gt;::percolateDown(int hole){ int child; Type tmp = array[hole]; for (; hole * 2 &lt;= currentSize; hole = child) { child = hole * 2; //如果有右孩子，且右孩子小于左孩子，则将右孩子赋值给child if (child != currentSize &amp;&amp; array[child + 1] &lt; array[child]) child++; if (array[child] &lt; tmp) array[hole] = array[child]; else break; } array[hole] = tmp;} 构造（建堆） 构造堆的时间复杂度可以为O(N)（只考虑最后的有序性，即在全部插入完后才调整） 为什么呢？因为对于一棵高度为h，包含了N=2^h+1^-1个结点的满二叉树，结点的高度和为N–h–1，说明操作数小于N证明：\\(s = \\sum_{i=0}^h 2^{i}(h-i)=N-h+1\\) 调整过程：左子堆和右子堆递归调用buildHeap（建堆的函数），对根结点调用percolateDown=&gt;也就是从编号最大的非叶结点开始percolateDown，逆向向上 template &lt;class Type&gt;void priorityQueue&lt;Type&gt;::buildHeap(){ for (int i = currentSize / 2; i &gt; 0; i--)//从下至上调整 percolateDown(i);} 来个例子：==注意：buildheap一次性建堆和一个空堆逐个插入数据是不一样的两个过程，一个全插入再调整位置，另一个一遍插入一遍调整位置== 应用因为是优先级队列，所以就是有优先级区分的排队系统的背景都可以用了第二部分12.13完成第三部分 集合结构第三部分是集合。==说实话感觉线性表和树在编程中似乎用的更多一些。==8.集合与静态查找表8.1集合的基本概念 集合中的数据元素除了属于同一集合之外，没有任何逻辑关系。 在集合中，每个数据元素有一个区别于其他元素的唯一标识，通常称为键值或关键字值 集合元素的实现 ==当时写程序题的时候没有看到这个东西（因为没听课），导致代码总缺点东西:Anguished:所以这边一定要把这东西放出来:triumph:，虽然不会考== template &lt;class KEY, class OTHER&gt;struct SET { KEY key;//关键字值 OTHER other;//元素内容}; 集合元素的存储 唯一一个仅适合于存储和处理集合的数据结构是==散列表== 8.2 查找 用于查找的集合称之为查找表查找表分类：静态查找表、动态查找表、内部查找、外部查找8.3无序表的查找（最无脑的）只能做线性（O(N)）的顺序查找==废话:triumph:，数据没顺序还怎么优化==8.4有序表的查找顺序查找按顺序查找即可，只是找不到的话不用查找到表头二分查找递归，按照中间值的偏差，即偏大or偏小选择左边/右边继续查找==这个东西其实能玩出很多花样的，懂的uu们都是懂的，但是个人认为应该考不到这么难==template &lt;class KEY, class OTHER&gt;int binarySearch(SET&lt;KEY, OTHER&gt; data[],int size, const KEY &amp;x){ int low = 1, high = size, mid; while (low &lt;= high) { // 查找区间存在 mid = (low + high) / 2; // 计算中间位置 if (x == data[mid].key) // 调整位置 return mid; if (x &lt; data[mid].key) high = mid - 1; else low = mid + 1; } return 0;}插值查找适用于数据的分布比较均匀的情况，外加访问元素比较费时的情况查找位置的估计：缺点：计算量大==我也不知道为什么，按理来说就算一次就行啦？:confused:====好吧，因为用的是浮点数计算，所以慢==分块查找处理大量数据查找的一种方法。它把整个有序表分成若干块，块内的数据元素可以是有序存储，也可以是无序的，但块之间必须是有序的。 查找由两个阶段组成：查找块，找到块后就可以查找索引9.动态查找表顾名思义，其既要支持快速查找，又要支持插入删除9.1二叉查找树9.1.1定义 左子树上的所有结点（非空）的关键字值均小于p结点的关键字值 右子树上的所有结点（非空）的关键字值均大于p结点的关键字值==可以让人联想到二分查找的过程==9.1.2实现和二叉树的链表实现差不多。不过任何操作都会从根节点开始查找、插入操作 查找：如果被查节点小于根节点，递归查找左子树，否则递归查找右子树（思路和二分查找类似），直到查找成功 插入：先查找，直到找到其父结点（若查找结点，发现结点按照指定方向递归下去时没有结点，这个结点即为父结点（==听不懂没有关系，我讲的很抽象==））。如果插入值小于根节点，插入到左子树，否则插入到右子树。插入后，新插入的结点总是叶子结点 template &lt;class KEY, class OTHER&gt;void BinarySearchTree&lt;KEY, OTHER&gt;::insert(const SET&lt;KEY,OTHER&gt; &amp;x)// 插入结点（外部调用函数）{ insert(x, root);// 调用内部insert函数}template &lt;class KEY, class OTHER&gt;void BinarySearchTree&lt;KEY, OTHER&gt;::insert(const SET&lt;KEY, OTHER&gt; &amp;x, BinaryNode *&amp;t) // 注意&amp;t这个引用符号，可以使插入结点与父结点建立关联{ if (t == NULL) t = new BinaryNode(x, NULL, NULL); else if (x.key &lt; t-&gt;data.key) insert(x, t-&gt;left); else if (t-&gt;data.key &lt; x.key) insert(x, t-&gt;right);} 删除操作 根据被删结点与根结点的大小关系，先查找到结点，然后开删分成三种情况： 叶结点直接删除（父结点相应指针为空） 有一个子结点子结点替代原有的父结点的位置，子结点下面不变 有两个子结点找出左子树中最大的结点或右子树中最小的结点来替代被删结点，所以就有两种选择此时用来替代的结点的位置空出来的，等同于结点被删除，因此继续分成上述3类进行调整（即问题递归） 9.1.3性能分析 正常情况下，如果二叉查找树平衡，那么时间复杂度为O(logN)（平均下来，查找时间在1.38logN，即O(logN)）在最坏的请况下，二叉查找树会退化为一个单链表。时间复杂度是O(N) 9.2 AVL树（二叉平衡树） 二叉平衡树是满足某个平衡条件的二叉查找树，其保证树的高度是O(logN)，从而操作都是O(logN) 最理想是每个节点的左右子树都有同样的高度，不过条件可以放宽一些，因此有了二叉平衡查找树9.2.1定义与条件平衡因子（平衡度）：结点的平衡度是结点的左子树的高度-右子树的高度空树的高度定义为-1要求每个结点的平衡因子都为+1（左边多）、-1（右边多）、0（每个结点的左右子树的高度最多差1）9.2.2代码实现采用二叉链表（即9.1中的），每个结点必须保存平衡信息（树的高度，从而能计算高度差）查找元素与9.1完全一样，也可以采取非递归的算法（就是将指针实时变化代替递归）（略，我觉得不重要）插入元素分为两种情况 没破坏平衡性：可以直接插入，然后自下而上修改结点平衡度（若有结点平衡度没变，上面的就都不用修改） 破坏了平衡性：需要调整树的结构（单旋转or双旋转），再修改平衡度 插入方法：（注：LL和RR、LR和RL是对称的问题，所以解决方法掌握其一即可） 因为原来的高度和旋转后的高度是一样的），所以调整一次即可```flowst=&gt;start: 从插入位置向根回溯op=&gt;operation: 重新计算平衡度op2=&gt;operation: 调整树的结构（分LL&amp;RR、LR&amp;RL两种）op5=&gt;operation: 节点处重新平衡e=&gt;end: 调整完毕 cond=&gt;condition: 节点平衡度是否合法？st-&gt;op-&gt;condop2-&gt;op5op5-&gt;econd(yes)-&gt;econd(no)-&gt;op21. **LL&amp;RR处理方法**![](https://notes.sjtu.edu.cn/uploads/upload_1a7fc5dcc754839a78bf2bda933a55ae.png =600x)```cpp//LLtemplate &lt;class KEY, class OTHER&gt;void AvlTree&lt;KEY, OTHER&gt;::LL(AvlNode *&amp;t){ AvlNode *t1 = t-&gt;left; // 未来的树根 t-&gt;left = t1-&gt;right; // 即BR的变化 // 危机节点的左子树变成了左子树的右子树 t1-&gt;right = t; // 左子树B的右子树变成了危机节点A t-&gt;height = max(height(t-&gt;left), height(t-&gt;right)) + 1; t1-&gt;height = max(height(t1-&gt;left), height(t)) + 1; // 高度更新 t = t1; // 危机节点的左子树变成了树根}RR方法是LL方法的对称，略 LR&amp;RL处理方法 //LRtemplate &lt;class KEY, class OTHER&gt;void AvlTree&lt;KEY, OTHER&gt;::LR(AvlNode *&amp;t){ //执行两次变换 RR(t-&gt;left); LL(t);} RL方法与LR方法对称，略 删除元素 与插入操作一样，失衡节点存在于被删节点到根节点的路径上 在删除了一个结点后，必须沿着到根结点的路径向上回溯，随时调整路径上的结点的平衡度。 删除操作没有插入操作那么幸运。插入时，最多只需要调整一个结点。而删除时，我们无法保证子树在平衡调整后的高度不变。只有当某个结点的高度在删除前后保持不变，才无需继续调整。 结点删除同二叉查找树。在删除了叶结点或只有一个孩子的结点后，子树变矮，则可能需要调整一共5种情况： 9.3散列表9.3.1哈希法哈希法：也称散列法。它不用比较的办法，而是直接根据所求结点的关键字值 KEY 找到这个结点。因此，它的时间复杂性为 O(1)。但不支持有关有序的操作 潜在问题：由于数据元素的关键字并不一定很小，为了解决这个问题，我们需要用一个将大数字映射成一个较小的、更容易管理的数字的函数来达到这个目的。将一个项映射成一个较小的下标的函数称为散列函数（hash function）。此外我们希望，能将字符串解释为一个整数（来扩大哈希法的用途） 9.3.2哈希函数 定义：每个结点在表中的存储位置是由一个函数H确定。该函数以结点的关键字值为参数，计算出该关键字对应的结点的存储位置。该函数称为哈希函数选择标准：计算速度快，散列地址尽可能均匀，使得冲突机会尽可能的少==以下的方法我觉得了解即可==# 常用的函数## 直接地址法- 直接关键字- H(key)=a×key+b（实际中真会取这种函数吗？）## 除留取余法- H(key)= key MOD p或 H(key)= key MOD p + c - 多选p为质数（而且比较大）理由：避免空间浪费## 数字分析法- 对关键字集合中的所有关键字，分析每一位上数字分布取数字分布均匀的位作为地址的组成部分## 平方取中法- 如果关键字中各位的分布都比较均匀，但关键字的值域比数组规模大则可以将关键字平方后，取其结果的中间各位作为散列函数值由于中间各位和每一位数字都有关系，因此均匀分布的可能性较大。## 折叠法- 选取一个长度后，将关键字按此长度分组相加条件：关键字相当长，以至于和散列表的单元总数相比大得多时，可采用此法9.3.3冲突的解决一般的哈希函数都是多对一。当两个以上的关键字映射到一个存储单元时，称为冲突或碰撞解决方法：==当然是找别的位置咯==（闭散列表）：1.线性探测法当散列发生冲突时，探测下一个单元，直到发现一个空单元查找：算出来位置之后，找不到的话就位置+1删除：一般来讲，删除某一元素，先要找到该元素，然后把该空间的内容清空。但这样就给查找带来了问题，某些元素会查不到==（ppt上这么说的，我不知道是啥意思）==。解决的方案是采用迟删除，即不真正删除元素，而是做一个删除标记2.二次探测法地址序列（冲突后）：这里有个定理：如果采用二次探测法，并且表的大小是一个素数，那么，如果表至少有一半是空的，新的元素总能被插入。而且，在插入过程中，没有一个单元被探测两次。（证明略）3.再次散列法采用第二个散列函数（再散列）。第i次碰撞时，地址为f(i) = hash1(x)+i×hash2(x)Hash2的选择是非常重要的。建议采用hash2(x) = R - (x mod R)，其中R是小于表长的一个素数（开散列表）：将碰撞的结点存放在散列表外的各自的线性表中（链接法） 散列表保存在一个数组中，数组的每个元素是一个指针，指向对应的单链表的首地址单链表不带头结点10.排序==排序这个东西为什么放在第三部分第十章，这是个值得思考的问题。个人认为应该属于线性表的内容，不过你说排序前元素的整体内部排列就和集合一样，我觉得也没问题。====排序本身内容还是很重要的，最好能熟练掌握==10.0部分概念 排序：把集合中的数据元素按照它们的关键字的非递减或非递增序排成一个序列 稳定与非稳定排序：多个关键字值相同的数据元素经过排序后，这些数据元素的相对次序保持不变，则稳定，反之则不稳定 内排序与外排序：内排序是指被排序的数据元素全部存放在计算机的内存之中，并且在内存中调整数据元素的相对位置。外排序是指在排序的过程中，数据元素主要存放在外存储器中，借助于内存储器逐步调整数据元素之间的相对位置。==（真会考这么细吗?!:confused:）==来一张各个排序的复杂度和稳定性的表格，大家可以了解一下，不要求背的（网上找的，仅供参考，选择排序应该是稳定的，另外基数排序这里有问题）10.1插入排序首先将由第一个数据元素组成的序列看成是有序的，然后将剩余的n-1个元素依次插入到前面的已排好序的子序列中去，使得每次插入后的子序列也是有序的。10.1.1直接插入排序 顾名思义，直接一个个进行比较，找到正确的地方，插入即可 时间复杂度是O(N^2^) for (int j = 1; j &lt; size; ++j){ tmp = a[j]; for (k = j - 1; tmp.key &lt; a[k].key &amp;&amp; k &gt;= 0; --k) a[k + 1] = a[k]; // 当插入的元素小于前面的元素时，将前面的元素后移 a[k + 1] = tmp; // 将tmp插入到合适的位置} 10.1.2折半插入排序 顾名思义，利用二分查找法，快速地找到a[j]的插入位置。从而达到减少比较次数的目的（O(logn)的级别） 最坏情况下总的移动次数还是O(n^2^)，这意味着找的效率高了，移动元素的效率没有高，故时间复杂度还是O(N^2^)==害，一个悲伤的故事==10.1.3希尔排序 插入排序算法的改进 从前面悲伤的故事，我们看到插排效率低主要是因为大量的移动 希尔排序先是比较那些离得稍远些的元素进行移动（从而不是一个一个移动交换），然后再不断缩小区间，逼近并变成直接插入排序 具体工作原理：设待排序的对象序列有n个对象，首先取一个整数gap &lt; n作为增量，将全部对象分为n/gap（向上取整）个子序列，所有距离为gap的对象放在同一个序列中，在每一个子序列中分别施行直接插入排序，然后缩小增量gap，重复上述的子序列划分和排序工作，直到最后取gap为1为止。==还是看图舒服==：（一步步缩小序列，最终成为直接插入排序）希尔建议gap从N/2开始平分直到gap为1，之后程序可终止。应用希尔增量，最坏的时间复杂性是O(N^2^)，平均时间复杂性是O(N^3/2^) 程序中的唯一关键点，就是多了个希尔增量step，控制插排的序列长度 for (step = size / 2; step &gt; 0; step /= 2) // step为希尔增量 for (i = step; i &lt; size; ++i) { tmp = a[i]; // 每次移动step步的插入排序 for (j = i - step; j &gt;= 0 &amp;&amp; a[j].key &gt; tmp.key; j -= step) a[j + step] = a[j]; a[j + step] = tmp; } 10.2选择排序从n个元素开始，每次从剩下的元素序列中选择关键字最小（也可以是最大）的元素，直至序列中最后只剩下一个元素为止。这样，把每次得到的元素排成一个序列，就得到了按非递减序排列的排序序列。10.2.1直接选择排序 时间复杂度O(N^2^)==请自行与插入排序进行对比，观察两者的区别（一个是拿来的就是最小的，一个是拿来然后找位置）== for (i = 0; i &lt; size - 1; ++i){ min = i; //找到剩下未排序的元素中的最小值 for (j = i + 1; j &lt; size; ++j) if (a[j].key &lt; a[min].key) min = j; tmp = a[i]; a[i] = a[min]; a[min] = tmp;} 10.2.2堆排序 核心：预处理数据，利用O(N)的步骤构建堆，实现O(logN)的查找效率直接选择排序在n个元素中选出最小元素需要n-1次比较。而利用基于堆的优先级队列选出最小元素只需要O(logN)的时间 排序N个元素，步骤如下：应用buildHeap对N个元素创建一个优先级队列通过调用N次deQueue取出每个项，结果就排好序了。 时间效益：建堆用了O(N)的时间，deQueue取出最小的项是对数时间。因此总的时间（N个数）是O(NlogN)。 注意与优先级队列中的堆有三个细小的区别：堆排序用的是最大堆为了和其他的排序函数保持一致，数据从位置0开始存储。因此，对位置i中的结点，父结点在位置（i – 1）/ 2，左孩子在位置2i + 1，右孩子紧跟着左孩子在向下过滤时需要告知当前堆的大小（需要额外传参）10.3交换排序根据序列中两个数据元素的比较结果来确定是否要交换这两个数据元素在序列中的位置。通过交换，将关键字值较大的数据元素向序列的尾部移动，关键字值较小的数据元素向序列的头部移动。10.3.1冒泡排序==曾经很喜欢用冒泡排序，因为过程容易理解而且稳定，后来发现sort()是真的方便（sort()是一个已经有的函数，不用自己写，采用快排，效率高，真香！）==从头到尾比较相邻的两个元素，将小的换到前面，大的换到后面。经过了从头到尾的一趟比较，就把最大的元素交换到了最后一个位置。这个过程称为一趟起泡。如此类推，经过第n-1趟起泡，将倒数第n-1个大的元素放入第2个单元。从而排序完成。 时间复杂度O(N^2^)（这个算法（为了出题）能玩出很多的花样，下面的flag就是一个例子，如果一次循环（起泡）没有交换，说明已经有序，那么就无需继续排序，可以直接结束，除此之外的优化技巧更是很多） bool flag = true; // 记录一趟起泡中有没有发生过交换for (i = 1; i &lt; size &amp;&amp; flag; ++i){ // size-1次起泡 flag = false;// 已经发生交换 for (j = 0; j &lt; size - i; ++j) // 第i次起泡 if (a[j + 1].key &lt; a[j].key)// 大小逆序，则顺序反了，发生交换 { tmp = a[j]; a[j] = a[j + 1]; a[j + 1] = tmp; flag = true; }} 10.3.2快速排序 在待排序的序列中选择一个数据元素，以该元素为标准，将所有数据元素分为两组，第一组的元素均小于或等于标准元素，第二组的数据元素均大于标准元素。第一组的元素放在数组的前面部分，第二组的数据元素放在数组的后面部分，标准元素放在中间。这个位置就是标准元素的最终位置。这称为一趟划分。然后对分成的两组数据重复上述过程，直到所有的元素都在适当的位置为止选择标准元素可能好（选到大小位于中间的元素）、可能坏（就是最大、最小的元素）具体代码实现：从右向左开始检查。如果high的值大于k，该位置中的值位置正确，high减1，继续往前检查，直到遇到一个小于k的值。将小于k的这个值放入low的位置。此时high的位置又空出来了。然后从low位置开始从左向右检查，直到遇到一个大于k的值。将low位置的值放入high位置，重复第一步，直到low和high重叠。将k放入此位置。划分完后，分成两份继续递归，直到划分到只有一个 平均情况下复杂度为O(NlogN)（但是最差会到O(N^2^)）```cpptemplate &lt;class KEY, class OTHER&gt;int divide(SET&lt;KEY, OTHER&gt; a[], int low, int high){ SET&lt;KEY, OTHER&gt; k = a[low]; do { while (low &lt; high &amp;&amp; a[high].key &gt;= k.key) // 从右向左找第一个大于k的元素 –high; if (low &lt; high) // 将小于k的元素放到左边 { a[low] = a[high]; ++low; } while (low &lt; high &amp;&amp; a[low].key &lt;= k.key) // 从左向右找第一个小于k的元素 ++low; if (low &lt; high) // 将大于k的元素放到右边 { a[high] = a[low]; –high; } } while (low != high); a[low] = k; // 将k放到中间 return low;}template &lt;class KEY, class OTHER&gt;void quickSort(SET&lt;KEY, OTHER&gt; a[], int low, int high){ int mid; if (low &gt;= high) // 如果只有一个元素，则直接返回 return; mid = divide(a, low, high); // 将数组分为左右两部分 quickSort(a, low, mid - 1); // 排序左一半 quickSort(a, mid + 1, high); // 排序右一半}### 10.4归并排序#### 10.4.1概念与思路归并排序的思想来源于合并两个已排序的有序表- 思想：如果N=1，已排序否则，对前一半和后一半分别调用归并排序归并两个已排序的数组采用分治法，用**递归**实现时间复杂度为O(NlogN)，但是需要更多的空间（O(N)）#### 10.4.2实现```cppint i = left, j = mid, k = 0;while (i &lt; mid &amp;&amp; j &lt;= right) // 两表都未结束 if (a[i].key &lt; a[j].key) tmp[k++] = a[i++]; else tmp[k++] = a[j++];while (i &lt; mid) tmp[k++] = a[i++]; // 前半部分没有结束while (j &lt;= right) tmp[k++] = a[j++]; // 后半部分没有结束for (i = 0, k = left; k &lt;= right;) a[k++] = tmp[i++];template &lt;class KEY, class OTHER&gt;void mergeSort(SET&lt;KEY, OTHER&gt; a[], int left, int right){ int mid = (left + right) / 2; if (left == right) return; mergeSort(a, left, mid); mergeSort(a, mid + 1, right); merge(a, left, mid + 1, right);}==实际操作中，其（二路归并排序）第一趟就是相邻两项比较排序==10.5基数排序称为口袋排序法，时间复杂度O(NlogN)通过分配的方法对整数进行排序以排序十进制非负整数为例，可设置10个口袋首先将元素按个位数分别放入十个口袋，然后将每个口袋中的元素倒出来按元素的十位数分别放入十个口袋。然后把它们倒出来再按百位数分配到最后一次倒出来时，元素就已经排好了序11.外部查找与排序==内容比较少，但是文字好多呀啊啊啊啊啊====大部分内容不用关心如何使用代码实现==11.1主存储器与外存储器 主存储器也被称为内存，是存储正在运行的程序代码及处理数据。 外存储器用于存储长期保存的信息。常用的外存储器有磁盘、磁带、光盘、U盘等，访问速度慢，故需考虑减少访问次数。外存储器中的信息以文件为单位。每个文件在内存有一个缓冲区存放正在处理的文件中的数据外存储器以数据块为单位与内存交换信息。当程序需要处理外存储器中的某个数据，则将包含该数据的数据块读入缓冲区进行处理 11.2外部查找 11.2.1 B树 ==理论太多了，精简不了了，尽量理解吧==想要把磁盘访问次数降低到一个很小的常数，比如说3或者4，我们可以增加树的分叉，就能降低树的高度，即采用M叉查找树（M叉查找树的最佳高度为：log~M~N）B树：B树是一棵平衡的M叉查找树，是索引存储中的索引结构记录可按添加到文件中的次序存在数据区中 一棵m阶B树或者为空，或者满足以下条件：根结点要么是叶子，要么至少有两个儿子，至多有m个儿子除根结点和叶子结点之外，每个结点的儿子个数 s 满足 M/2（向上取整）到M之间有s个儿子的非叶结点具有n=s-1个关键字，这些结点的数据信息为：（n, A0, (K1, R1), A1, (K2, R2), A2, ……… (Kn, Rn), An）所有的叶子结点都出现在同一层上，即它们的深度相同，并且不带信息11.2.2 B树的插入与二叉查找树类似，插入总是在最底层。首先在m阶B树上进行查找操作，确定新插入的关键字key在最底层的非叶结点的插入位置，将 key 和记录的存储地址按序插入到最底层上的某个结点。 若被插入结点的关键字个数小于等于m-1，则插入操作结束。若该结点原有的关键字个数已经等于m，必须分裂成两个结点。以上面为例，插入50后，关键字个数超了，于是从中间切开，中间的元素放在父节点，而剩下的分裂成2个子结点：11.2.3 B树的删除类似于二叉查找树的删除操作，同样采用了“替身”的方法 若不是最底层：替身为右子树最左面的关键字或左子树最右面的关键字 删除最底层的关键字，则有以下几种情况：若删除关键字之后，结点的关键字的个数满足B树的结点的定义，删除结束。若删除后关键字个数小于下限：向结点的左或右兄弟结点借一个关键字过来。若该结点的左或右兄弟结点的关键字的个数正好为下限，则合并结点 11.2.4 B树占用空间的情况（似乎没有特别重要） 将一个磁盘块作为一个B树的结点。假设一个块的容量max字节如果每个键要占用key个字节。在一棵M阶B树中，可以有M-1个键，总的数据量是(M-1)* key + M个分支的地址 + （M-1个关键字对应记录的存储地址） 11.2.5 B+树 B+树是既能提供随机查找，也能提供顺序访问的存储结构。（B树不适合顺序访问）M阶的B+树是具有以下性质的M叉树：数据记录被存贮在叶子中。非叶子结点至多保存M-1个键来引导查找，键i表示子树i+1中键的最小值。根有2到M个儿子。除根之外所有的非叶结点的儿子数为M/2（向上取整）到M之间。这保证了B树不会退化成二叉树。所有的叶子都在同一层上，并且对于某个L要有M/2（向上取整）到L个数据项 所有的叶子结点连成一个单链表 11.2.6 B+树的插入 叶结点不满：把新节点插入叶子，重新调整该叶子中数据的顺序 叶子已经装满 ：通过分裂该叶子，形成两个半满的叶子来插入一个新的项，并更新父节点如果父亲的儿子数量已经满了，我们就继续分裂父亲。最坏情况要分裂根（这就是为什么根节点允许只有两个孩子） 11.2.7 B+树的删除 删除操作首先查找到要删除的项，然后删除它 如果此时它所在的叶子的元素数量正好满足要求的最小值，删除该项就会使它低于最小值 如果邻居不是最少的情况，就借一个过来领养；如果邻居也处于最少的情况，就把两个结点合并成一个满的结点。很不幸的是，在这种情况下父亲就失去了一个儿子。如果它引起父亲的儿子数少于了最小值，我们就要使用同样的策略了。这个过程一直向上进行过滤到根。如果在寄养的过程中，根只剩下了一个儿子，就把根删除，让它的儿子作为新的树根，这也是唯一能使B树变矮的情况。 似乎可以做个思维导图 11.3外排序 11.3.1总述 由于一次外存操作所需的时间可以执行数百条甚至上千条指令，因此在外排序中主要考虑的是如何减少外存储器的读写 在外存上进行排序的最常用的方法是利用归并排序，因为归并排序只需要访问被归并序列中的第一个元素，这非常适合于顺序文件。外排序由两个阶段组成，下面一一介绍 11.3.2预处理阶段 预处理阶段：根据内存的大小将一个有n个记录的文件分批读入内存，用各种内排序算法排序，形成一个个有序片段。如果能够让每个初始的已排序片段包含更多的记录，就能减少排序时间。置换选择法可以在只能容纳p个记录的内存中生成平均长度为2p的初始的已排序片段。 至于如何置换的，看图就行了，下面的例子中内存只能容纳3个记录。 从输入磁带读入下一个元素。如果它比刚才写出去的元素大，则把它加入到优先级队列；否则，它不可能进入当前的已排序片段。因为优先级队列比以前少了一个元素，该元素就被放于优先级队列的空余位置（用于下个片段的排序）根据数据的特点，我们能排一段是一段，一直到文件结束 11.3.3归并阶段 归并阶段：将这些有序片段逐步归并成一个有序文件。 归并时，每次将两个有序文件归并成一个有序文件如果生成的有序片段数是M，则归并次数为log~2~M（向上取整）以下是一个例子，M=3从输入磁带上一次读入M个记录，对它们进行内排序，然后把已排序片段轮流写到B1和B2。回绕所有的磁带。（注，以下地方原有错误，已经修复）取每条磁带上的第一个已排序片段，把它们归并起来，并把结果写到A1。然后，从每条磁带上取下一个已排序片段，把它们归并起来，结果写到A2。继续这个过程，轮流把结果写到A1和A2， 回绕四条磁带，重复同样的步骤，这次使用A磁带作为输入，而B磁带作为输出。重复步骤二和三，直到剩下一个长度为N的已排序片断 多路归并 同时将k个有序文件归并成一个优点：减少归并次数，只需要log~k~m次归并缺点：归并时找最小元素的操作复杂，通常可以将每个文件的第一个记录组成一个优先级队列。K路归并需要2k条磁带。A~1~到A~k~和B~1~到B~k~。归并数据在A上过程：==就是把2条变成k条而已==回绕2k根磁带归并A~1~到A~k~条磁带上的有序片段轮流放入B1到Bk回绕所有磁带归并B~1~到B~k~条磁带上的有序片段轮流放入A1到Ak重复上述过程，直到只剩下一个有序片段 多阶段归并 磁带上的K路归并策略需要用2K条磁带，这可能限制了它在某些应用中的使用。可以仅用K+1根磁带实现K路归并，这称为多阶段归并==也就是比如3个磁带，我拿一个片段+另一个片段的一部分归并，然后轮流归并==下图：T2的13段与T3的13段归并存到T1，T1中8段和T2中8段归并存到T3，以此类推 如何分配：如果已排序片段的数目是一个斐波纳契数FN，那么分布最好的方法把它们分解成两个斐波纳契数FN-1和FN-2。否则，为了将已排序片段数增加到一个斐波纳契数，必须在磁带上填充虚拟的已排序片段。==第12章应该没学==第四部分 图状结构==最后一个部分啦==13.图13.1图的定义图可以用G=(V,E)表示。其中，V是顶点的集合，E是连接顶点的边（弧）的集合。如果边是有方向的，称为有向图。有向图的边用&lt;&gt;表示。&lt;A,B&gt;表示从A出发到B的一条边。在有向图中，&lt;A,B&gt;和&lt;B,A&gt;是不一样的。V = {A,B,C,D},E = {&lt;A,B&gt;,&lt;B,A&gt;,&lt;A,C&gt;,&lt;C,A&gt;,&lt;C,D&gt;,&lt;D,A&gt;}表示的图如下所示如果边是无方向的，称为无向图。无向图的边通常用圆括号表示。（A，B）表示顶点A和B之间有一条边。无向图也称为双向图。V = {A,B,C,D,E}E = {(A,B),(A,C),(B,D),(B,E),(D,E),(C,E)}加权图：边被赋予一个权值的图称为加权图。如果图是有向的，称为加权有向图，如果是无向的，称为加权无向图。13.2图的术语==概念有点多啊==# 图的术语## 区分有向图无向图### 有向图#### 邻接- &lt;Vi,Vj&gt;是图中的一条边，则称**Vi邻接到Vj**，或Vj和Vi邻接#### 入度- 有向图中进入某一结点的边数，称为该结点的入度#### 出度- 有向图中离开某一结点的边数，称为该结点的出度#### 连通性- 强连通图：有向图 G 的任意两点之间都是连通的，则称 G 是强连通图。- 强连通分量：极大连通子图- 弱连通图：如有向图G不是强连通的，但如果把它看成是无向图时是连通的，则称该图是弱连通的- 有向无环图：图中没有环，简写为DAG#### 有向完全图- 有向完全图：每两个节点之间都有两条弧的有向图称为有向完全图。有向完全图有n(n-1)（排列组合2C2n）条边。其中n是结点个数### 无向图#### 邻接- (Vi,Vj)是图中的一条边，则称Vi和Vj是邻接的#### 度- 无向图中邻接于某一结点的边的总数#### 连通性- 连通：顶点v至v’ 之间有路径存在- 连通图：无向图 G 的任意两点之间都是连通的，则称 G 是连通图。- 连通分量：非连通图中的极大连通子图#### 完全图- 每两个节点之间都有边的无向图称为**完全图**。完全图有n(n-1)/2（排列组合C2n）条边。其中n是结点个数## 不区分### 子图- 边&amp;顶点均为另一个图的子集- 顾名思义### 路径- 对1&lt;i&lt;N，结点序列w1,w2,……wN 中的结点对（wi, wi+1）都有（wi, wi+1）∈ E或&lt;wi, wi+1&gt; ∈ E那么，w1,w2,……wN是图中的一条路径。#### 非加权的路径长度- 组成路径的边数，对于路径w1,w2,……wN，非加权路径长度为N-1#### 加权路径长度- 路径上所有边的权值之和。#### 简单路径- 一条路径上的所有结点，除了起始结点和终止结点可能相同外，其余的结点都不相同#### 回路（环）- 是一条简单路径，其起始结点和终止结点相同，且路径长度至少为1还有个生成树连通图的极小连通子图。包含图的所有 n 个结点，但只含图的n-1条边。在生成树中添加一条边之后，必定会形成回路或环。==是不是有点眼熟，因为在电路理论里面见过，就是电路图中的树，加一个连支形成回路==13.3图的存储13.3.1（加权）邻接矩阵 有向图：设有向图具有n个结点，则用n行n列的布尔矩阵A表示该有向图如果i至j有一条有向边, A[i,j]=1 ,如果 i 至 j 没有一条有向边,A[i,j]=0实际实现：分别用 0、1、2、3 分别标识结点A、B、C、D。而将真正的数据字段之值放入一个一维数组之中 无向图：具有n 个结点，则用n行n列的布尔矩阵A表示该无向图i至j有一条无向边：A[i,j]=1；i至j没有一条无向边：A[i,j]=0实际实现和有向图类似 加权有向图（无向图同理）：设有向图具有n个结点，则用n行n列的矩阵A表示该有向图； 如果i至j有一条有向边且它的权值为a ，则A[i,j]=a 。如果i至j没有一条有向边。则A[i,j]=空或其它标志 邻接矩阵的特点：优点：寻找边仅需O(1)缺点：空间复杂度O(n^2)，在边数少的时候十分致命 13.3.2邻接表 设有向图或无向图具有 n 个结点，则用结点表、边表表示该有向图或无向图。结点表：用数组或单链表的形式存放所有的结点值。如果结点数n已知，则采用数组形式，否则应采用单链表的形式。边表（边结点表）：每条边用一个结点进行表示。同一个结点出发的所有的边形成它的边结点单链表。 邻接表是图的标准存储方式 优点： 内存 ＝ 结点数 ＋ 边数，处理时间也是结点数 ＋ 边数，即为O( V + E )。 当谈及图的线性算法时，一般指的是O( V + E ) 缺点：确定 i –&gt; j 是否有边，最坏需耗费 O(n) 时间。无向图同一条边表示两次。边表空间浪费一倍。有向图中寻找进入某结点的边，非常困难。 13.4图的遍历 13.4.1深度优先搜索 ==就是一条路走到黑，然后如果没遍历完，那就回去找别的路继续走到黑……，直到遍历结束==1、选中第一个被访问的顶点；2、对顶点作已访问过的标志；3、依次从顶点的未被访问过的第一个、第二个、第三个…… 邻接顶点出发，进行深度优先搜索；4、如果还有顶点未被访问，则选中一个起始顶点，转向2；5、所有的顶点都被访问到，则结束。这样遍历会获得一棵深度优先生成树在进行深度优先搜索 DFS 时，有时并不一定能够保证从某一个结点出发能访问到所有的顶点在这种情况下，必须再选中一个未访问过的顶点，继续进行深度优先搜索。直至所有的顶点都被访问到为止。这时会生成深度优先生成森林 示例：for (int i = 0; i &lt; Vers; ++i) visited[i] = false;cout &lt;&lt; \"当前图的深度优先遍历序列为：\" &lt;&lt; endl;for (i = 0; i &lt; Vers; ++i){ if (visited[i] == true) continue; dfs(i, visited); cout &lt;&lt; endl;}void dfs(v,visited){ visited[v] = true; for 每个 v的邻接点w if (!visited[w]) dfs(w, visited);//递归调用}广度优先搜索类似于树的从树根出发的按照层次的遍历。==是一个点，先把这个点能走的路都走一遍，而且就走一下，然后再继续到下一点遍历==1、选中第一个被访问的顶点；2、对顶点作已访问过的标志；3、依次访问已访问顶点的未被访问过的第一个、第二个、第三个……第 m 个邻接顶点 W1 、W2、W3…… Wm ，进行访问且进行标记，转向3；4、如果还有顶点未被访问，则选中一个起始顶点，转向2；5、所有的顶点都被访问到，则结束。 示例：例如，从5开始广度优先搜索这个图，得到的遍历序列为：5，6，7，2，4，3，1。 ![]bool *visited = new bool[Vers];int currentNode;linkQueue&lt;int&gt; q;edgeNode *p;for (int i = 0; i &lt; Vers; ++i) visited[i] = false;cout &lt;&lt; \"当前图的广度优先遍历序列为：\"&lt;&lt; endl;for (i = 0; i &lt; Vers; ++i){ if (visited[i] == true) continue; q.enQueue(i); while (!q.isEmpty()) { currentNode = q.deQueue(); if (visited[currentNode] == true)//访问过则跳过 continue; cout &lt;&lt; verList[currentNode].ver &lt;&lt; '\\t'; visited[currentNode] = true; p = verList[currentNode].head; while (p != NULL) { if (visited[p-&gt;end] == false)//未访问过则入队 q.enQueue(p-&gt;end); p = p-&gt;next; } } cout &lt;&lt; endl;}==两个的区别：一个的调用结构是栈，另一个是队列==时间复杂度：两个函数将对所有的顶点和边进行访问，因此它的时间代价和顶点数 |V| 及边数 |E| 是相关的，即是O(|V|+|E|)。如果图是用邻接矩阵来表示，则所需要的时间是O（|V|^2^）13.5图遍历的应用13.5.1无向图的连通性深度优先搜索和广度优先搜索都可以用来测试无向图的连通性。如果无向图是连通的，则从无向图中的任意结点出发进行深度优先搜索或广度优先搜索都可以访问到每一个结点。13.5.2欧拉回路如果能够在一个图中找到一条路径，使得该路径对图的每一条边正好经过一次，这条路径被称为欧拉路径。如果再增加一个附加条件，即起点和终点是相同的，这条路径被称为欧拉回路。欧拉回路问题也被称为一笔画问题。欧拉回路的性质如果有奇数桥的地方不止两个，满足要求的路径是找不到的。如果只有两个地方有奇数桥，可以从这两个地方之一出发，经过所有的桥一次，再回到另一个地方。如果都是偶数桥，从任意地方出发都能回到原点。使用dfs进行遍历执行一次深度优先的搜索。从起始结点开始，沿着这条路一直往下走，直到无路可走。而且在此过程中不允许回溯。如果遍历完了发现边没有走完，则找出路径上的另外一个尚有未访问的边的顶点，开始另一次深度优先的搜索，将得到的遍历序列拼接到原来的序列中，直到所有的边都已被访问。代码实现（自己看别的ppt吧，代码有点多了）13.5.3有向图的连通性对有向图，深度优先搜索可以测试是否强连通，并找出所有强连通分量找强连通分量的方法从任意节点开始深度优先遍历G，得到深度优先森林。对每一棵树按照生成顺序依次进行后续遍历，按照遍历顺序记录每个节点编号将G的每条边逆向，形成Gr。从编号最大的节点开始深度优先遍历Gr。得到的深度优先遍历森林的每棵树就是G的强连通分量。13.5.4拓扑排序结果不一定唯一，所以代码实现应该只能输出其中的一种情况过程：序列中第一个元素必须无前驱（入度为0）后驱：必须等到其前驱输出后才能输出如果图以邻接表表示：O（V+E）（V为寻找入度为0节点，E为搜索后续节点）可行的排课：方案1： 1，2，3，4，5，6，7方案2： 1，2，3，5，6，4，7方案3： 1，2，3，5，6，7，4……实现计算每个结点的入度，保存在数组inDegree中；检查inDegree中的每个元素，将入度为0的结点入队；不断从队列中将入度为0的结点出队，输出此结点，并将该结点的后继结点的入度减1；如果某个邻接点的入度为0，则将其入队。下图：从左到右依次为各课程入度随程序运行的变化template &lt;class TypeOfVer, class TypeOfEdge&gt;void adjListGraph&lt;TypeOfVer, TypeOfEdge&gt;::topSort() const{ linkQueue&lt;int&gt; q; edgeNode *p; int current, *inDegree = new int[Vers]; for (int i = 0; i &lt; Vers; ++i) inDegree[i] = 0; for (i = 0; i &lt; Vers; ++i) for (p = verList[i].head; p != NULL; p = p-&gt;next) ++inDegree[p-&gt;end];//计算入度 for (i = 0; i &lt; Vers; ++i) if (inDegree[i] == 0) q.enQueue(i);//入队入度为0的顶点 cout &lt;&lt; \"拓扑排序为：\" &lt;&lt; endl; while (!q.isEmpty()) { current = q.deQueue(); cout &lt;&lt; verList[current].ver &lt;&lt; '\\t'; for (p = verList[current].head; p != NULL; p = p-&gt;next) if (--inDegree[p-&gt;end] == 0)//减去入度，如果入度为0，则入队 q.enQueue(p-&gt;end); } cout &lt;&lt; endl;}13.5.5关键路径AOE网络：顶点表示事件，有向边权值代表时间，指向本身代表顺序（有向图）完成整项工程至少需要多少时间，从源点到收点最长路径的长度，称为关键路径==也就是说，关键路径是最长的（即花费最多的）路径，而非最短的在实际生活中，可以把节点认定为生产的一道道工序==几个概念：最早发生时间：（前驱节点最早时间+边）的最大值，否则会浪费时间，从而影响整体以上面为例子：最晚发生时间：（后继节点最晚时间-边）的最小值，否则会耽搁进程，影响整体完成时间（即关键路径）最早最晚时间相等，就是关键路径上的顶点（==意思就是说，关键路径上的节点没有可以选择发生的时间区间，一旦需要发生就会直接发生（听不懂也没关系）==）实现：在拓扑排序后，依据排序结果遍历节点，寻找min的值==后面的应该都没学====完结撒花:tada:=="
    } ,
  
    {
      "title"       : "Who owns the copyright for an AI generated creative work?",
      "category"    : "opinion",
      "tags"        : "copyright, creativity, neural networks, machine learning, artificial intelligence",
      "url"         : "./AI-and-intellectual-property.html",
      "date"        : "2021-04-20 00:00:00 +0800",
      "description" : "As neural networks are used more and more in the creative process, text, images and even music are now created by AI, but who owns the copyright for those works?",
      "content"     : "Recently I was reading an article about a cool project that intends to have a neural network create songs of the late club of the 27 (artists that have tragically died at age 27 or near, and in the height of their respective careers), artists such as Amy Winehouse, Jimmy Hendrix, Curt Cobain and Jim Morrison.The project was created by Over the Bridge, an organization dedicated to increase awareness on mental health and substance abuse in the music industry, trying to denormalize and remove the glamour around such illnesses within the music community.They are using Google’s Magenta, which is a neural network that precisely was conceived to explore the role of machine learning within the creative process. Magenta has been used to create a brand new “Beatles” song or even there was a band that used it to write a full album in 2019.So, while reading the article, my immediate thought was: who owns the copyright of these new songs?Think about it, imagine one of this new songs becomes a massive hit with millions of youtube views and spotify streams, who can claim the royalties generated?At first it seems quite simple, Over the Bridge should be the ones reaping the benefits, since they are the ones who had the idea, gathered the data and then fed the neural network to get the “work of art”. But in a second thought, didn’t the original artists provide the basis for the work the neural network generated? shouldn’t their state get credit? what about Google whose tool was used, should they get credit too?Neural networks have been also used to create poetry, paintings and to write news articles, but how do they do it? A computer program developed for machine learning purposes is an algorithm that “learns” from data to make future decisions. When applied to art, music and literary works, machine learning algorithms are actually learning from some input data to generate a new piece of work, making independent decisions throughout the process to determine what the new work looks like. An important feature of this is that while programmers can set the parameters, the work is actually generated by the neural network itself, in a process akin to the thought processes of humans.Now, creative works qualify for copyright protection if they are original, with most definitions of originality requiring a human author. Most jurisdictions, including Spain and Germany, specifically state that only works created by a human can be protected by copyright. In the United States, for example, the Copyright Office has declared that it will “register an original work of authorship, provided that the work was created by a human being.”So as we currently stand, a human author is required to grant a copyright, which makes sense, there is no point of having a neural network be the beneficiary of royalties of a creative work (no bank would open an account for them anyways, lol).I think amendments have to be made to the law to ensure that the person who undertook all the arrangements necessary for the work to be created by the neural network gets the credit but also we need to modify copyright law to ensure the original authors of the body of work used as data input to produce the new piece get their corresponding share of credit. This will get messy if someone uses for example the #1 song of every month in a decade to create the decade song, then there would be as many as 120 different artists to credit.In a computer generated artistic work, both the person who undertook all the arrangements necessary for its creation as well as the original authors of the data input need to be credited.There will still be some ambiguity as to who undertook the arrangements necessary, only the one who gathered the data and pressed the button to let the network learn, or does the person who created the neural network’s model also get credit? Shall we go all the way and say that even the programmer of the neural network gets some credit as well?There are some countries, in particular the UK where some progress has been made to amend copyright laws to cater for computer generated works of art, but I believe this is one of those fields where technology will surpass our law making capacity and we will live under a grey area for a while, and maybe this is just what we need, by having these works ending up free for use by anyone in the world, perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living, and thus they can become free to explore their art.Perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living.The Next Rembrandt is a computer-generated 3-D–printed painting developed by a facial-recognition algorithm that scanned data from 346 known paintings by the Dutch painter in a process lasting 18 months. The portrait is based on 168,263 fragments from Rembrandt’s works."
    } ,
  
    {
      "title"       : "So, what is a neural network?",
      "category"    : "theory",
      "tags"        : "neural networks, machine learning, artificial intelligence",
      "url"         : "./back-to-basics.html",
      "date"        : "2021-04-02 00:00:00 +0800",
      "description" : "ELI5: what is a neural network.",
      "content"     : "The omnipresence of technology nowadays has made it commonplace to read news about AI, just a quick glance at today’s headlines, and I get: This Powerful AI Technique Led to Clashes at Google and Fierce Debate in Tech. How A.I.-powered companies dodged the worst damage from COVID AI technology detects ‘ticking time bomb’ arteries AI in Drug Discovery Starts to Live Up to the Hype Pentagon seeks commercial solutions to get its data ready for AITopics from business, manufacturing, supply chain, medicine and biotech and even defense are covered in those news headlines, definitively the advancements on the fields of artificial intelligence, in particular machine learning and deep neural networks have permeated into our daily lives and are here to stay. But, do the general population know what are we talking about when we say “an AI”? I assume most people correctly imagine a computer algorithm or perhaps the more adventurous minds think of a physical machine, an advanced computer entity or even a robot, getting smarter by itself with every use-case we throw at it. And most people will be right, when “an AI” is mentioned it is indeed an algorithm run by a computer, and there is where the boundary of their knowledge lies.They say that the best way to learn something is to try to explain it, so in a personal exercise I will try to do an ELI5 (Explain it Like I am 5) version of what is a neural network.Let’s start with a little history, humans have been tinkering with the idea of an intelligent machine for a while now, some even say that the idea of artificial intelligence was conceived by the ancient greeks (source), and several attempts at devising “intelligent” machines have been made through history, a notable one was ‘The Analytical Engine’ created by Charles Babbage in 1837:The Analytical Engine of Charles Babbage - 1837Then, in the middle of last century by trying to create a model of how our brain works, Neural Networks were born. Around that time, Frank Rosenblatt at Cornell trying to understand the simple decision system present in the eye of a common housefly, proposed the idea of a perceptron, a very simple system that processes certain inputs with basic math operations and produces an output.To illustrate, let’s say that the brain of the housefly is a perceptron, its inputs are whatever values are produced by the multiple cells in its eyes, when the eye cell detects “something” it’s output will be a 1, and if there is nothing a 0. Then the combination of all those inputs can be processed by the perceptron (the fly brain), and the output is a simple 0 or 1 value. If it is a 1 then the brain is telling the fly to flee and if it is a 0 it means it is safe to stay where it is.We can imagine then that if many of the eye cells of the fly produce 1s, it means that an object is quite near, and therefore the perceptron will calculate a 1, it is time to flee.The perceptron is just a math operation, one that multiplies certain input values with preset “parameters” (called weights) and adds up the resulting multiplications to generate a value.Then the magic spark was ignited, the parameters (weights) of the perceptron could be “learnt” by a process of minimizing the difference between known results of particular observations, and what the perceptron is actually calculating. It is this process of learning what we call training the neural network.This idea is so powerful that even today it is one of the fundamental building blocks of what we call AI.From this I will try to explain how this simple concept can have such diverse applications as natural language processing (think Alexa), image recognition like medical diagnosis from a CTR scan, autonomous vehicles, etc.A basic neural network is a combination of perceptrons in different arrangements, the perceptron therefore was downgraded from “fly brain” to “network neuron”.A neural network has different components, in its basic form it has: Input Hidden layers OutputInputThe inputs of a neural network are in their essence just numbers, therefore anything that can be converted to a number can become an input. Letters in a text, pixels in an image, frequencies in a sound wave, values from a sensor, etc. are all different things that when converted to a numerical value serve as inputs for the neural network. This is one of the reasons why applications of neural networks are so diverse.Inputs can be as many as one need for the task at hand, from maybe 9 inputs to teach a neural network how to play tic-tac-toe to thousands of pixels from a camera for an autonomous vehicle. Since the input of a perceptron needs to be a single value, if for example a color pixel is chosen as input, it most likely will be broken into three different values; its red, green and blue components, hence each pixel will become 3 different inputs for the neural network.Hidden layersA “layer” within a neural network is just a group of perceptrons that all perform the same exact mathematical operation to the inputs and produce an output. The catch is that each of them have different weights (parameters), therefore their output for a given input will be different amongst them. There are many types of layers, the most typical of them being a “dense” layer, which is another word to say that all the inputs are connected to all the neurons (individual perceptrons), and as said before, each of these connections have a weight associated with it, so that the operation that each neuron performs is a simple weighted sum of all the inputs.The hidden layer is then typically connected to another dense layer, and their connection means that each output of a neuron from the first layer is treated effectively as an input for the subsequent one, and it is thus connected to every neuron.A neural network can have from one to as many layers as one can think, and the number of layers depends solely on the experience we have gathered on the particular problem we would like to solve.Another critical parameter of a hidden layer is the number of neurons it has, and again, we need to rely on experience to determine how many neurons are needed for a given problem. I have seen networks that vary from a couple of neurons to the thousands. And of course each hidden layer can have as many neurons as we please, so the number of combinations is vast.To the number of layers, their type and how many neurons each have, is what we call the network topology (including the number of inputs and outputs).OutputAt the very end of the chain, another layer lies (which behaves just like a hidden layer), but has the peculiarity that it is the final layer, and therefore whatever it calculates will be the output values of the whole network. The number of outputs the network has is a function of the problem we would like to solve. It could be as simple as one output, with its value representing a probability of an action (like in the case of the flee reaction of the housefly), to many outputs, perhaps if our network is trying to distinguish images of animals, one would have an output for each animal species, and the output would represent how much confidence the network has that the particular image belongs to the corresponding species.As we said, the neural network is just a collection of individual neurons, doing basic math operations on certain inputs in series of layers that eventually generate an output. This mesh of neurons is then “trained” on certain output values from known cases of the inputs; once it has learned it can then process new inputs, values that it has never seen before with surprisingly accurate results.Many of the problems neural networks solve, could be certainly worked out by other algorithms, however, since neural networks are in their core very basic operations, once trained, they are extremely efficient, hence much quicker and economical to produce results.There are a few more details on how a simple neural network operate that I purposedly left out to make this explanation as simple as possible. Thinks like biases, the activation functions and the math behind learning, the backpropagation algorithm, I will leave to a more in depth article. I will also write (perhaps in a series) about the more complex topologies combining different types of layers and other building blocks, a part from the perceptron.Things like “Alexa”, are a bit more complex, but work on exactly the same principles. Let’s break down for example the case of asking “Alexa” to play a song in spotify. Alexa uses several different neural networks to acomplish this:1. Speech recognitionAs a basic input we have our speech: the command “Alexa, play Van Halen”. This might seem quite simple for us humans to process, but for a machine is an incredible difficult feat to be able to understand speech, things like each individual voice timbre, entonation, intention and many more nuances of human spoken language make it so that traditional algorithms have struggled a lot with this. In our simplified example let’s say that we use a neural network to transform our spoken speech into text characters a computer is much more familiarized to learn.2. Understanding what we mean (Natural Language Understanding)Once the previous network managed to succesfuly convert our spoken words into text, there comes the even more difficult task of making sense of what we said. Things that we humans take for granted such as context, intonation and non verbal communication, help give our words meaning in a very subtle, but powerful way, a machine will have to do with much less information to correctly understand what we mean. It has to correctly identify the intention of our sentence and the subject or entities of what we mean.The neural network has to identify that it received a command (by identifying its name), the command (“play music”), and our choice (“Van Halen”). And it does so by means of simple math operations as described before. Of course the network involved is quite complex and has different types of neurons and connection types, but the underlying principles remain.3. Replying to usOnce Alexa understood what we meant, it then proceeds to execute the action of the command it interpreted and it replies to us in turn using natural language. This is accomplished using a technique called speech synthesis, things like pitch, duration and intensity of the words and phonems are selected based on the “meaning” of what Alexa will respond to us: “Playing songs by Van Halen on Spotify” sounding quite naturally. And all is accomplished with neural networks executing many simple math operations.Although it seems quite complex, the process for AI to understand us can be boiled down to simple math operationsOf course Amazon’s Alexa neural networks have undergone quite a lot of training to get to the level where they are, the beauty is that once trained, to perform their magic they just need a few mathematical operations.As said before, I will continue to write about the basics of neural networks, the next article in the series will dive a bit deeper into the math behind a basic neural network."
    } ,
  
    {
      "title"       : "Starting the adventure",
      "category"    : "",
      "tags"        : "general blogging, thoughts, life",
      "url"         : "./starting-the-adventure.html",
      "date"        : "2021-03-24 00:00:00 +0800",
      "description" : "Midlife career change: a disaster or an opportunity?",
      "content"     : "In the midst of a global pandemic caused by the SARS-COV2 coronavirus; I decided to start blogging. I wanted to blog since a long time, I have always enjoyed writing, but many unknowns and having “no time” for it prevented me from taking it up. Things like: “I don’t really know who my target audience is”, “what would my topic or topics be?”, “I don’t think I am a world-class expert in anything”, and many more kept stopping me from setting up my own blog. Now seemed like a good time as any so with those and tons of other questions in my mind I decided it was time to start.Funnily, this is not my first post. The birth of the blog came very natural as a way to “document” my newly established pursuit for getting myself into Machine Learning. This new adventure of mine comprises several things, and if I want to succeed I need to be serious about them all: I want to start coding again! I used to code a long time ago, starting when I was 8 years old in a Tandy Color Computer hooked up to my parent’s TV. Machine Learning is a vast, wide subject, I want to learn the generals, but also to select a few areas to focus on. Setting up a blog to document my journey and share it: Establish a learning and blogging routine. If I don’t do this, I am sure this endeavour will die off soon.As for the focus areas I will start with: Neural Networks fundamentals: history, basic architecture and math behind them Deep Neural Networks Reinforcement Learning Current state of the art: what is at the cutting edge now in terms of Deep Neural Networks and Reinforcement Learning?I selected the above areas to focus on based on my personal interests, I have been fascinated by the developments in reinforcement learning for a long time, in particular Deep Mind’s awesome Go, Chess and Starcraft playing agents. Therefore, I started reading a lot about it and even started a personal project for coding a tic-tac-toe learning agent.With my limited knowledge I have drafted the following learning path: Youtube: Three Blue One Brown’s videos on Neural Networks, Calculus and Linear Algebra. I cannot recommend them enough, they are of sufficient depth and use animation superbly to facilitate the understanding of the subjects. Coursera: Andrew Ng’s Machine Learning course Book: Deep Learning with Python by Francois Chollet Book: Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. BartoAs for practical work I decided to start by coding my first models from scratch (without using libraries such as Tensorflow), to be able to deeply understand the math and logic behind the models, so far it has proven to be priceless.For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.I will continue to write about my learning road, what I find interesting and relevant, and to document all my practical exercises, as well as news and the state of the art in the world of AI.So far, all I have learned has been so engaging that I am seriously thinking of a career change. I have 17 years of international experience in multinational corporations across various functions, such as Information Services, Sales, Customer Care and New Products Introduction, and sincerely, I am finding more joy in artificial intelligence than anything else I have worked on before. Let’s see where the winds take us.Thanks for reading!P.S. For the geeks like me, here is a snippet on the technical side of the blog.Static Website GeneratorI researched a lot on this, when I started I didn’t even know I needed a static website generator. I was just sure of one thing, I wanted my blog site to look modern, be easy to update and not to have anything extra or additional content or functionality I did not need.There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are: wordpress wix squarespace ghost webflow netlify hugo gatsby jekyllI started with the web interfaced generators with included hosting in their offerings:wordpress is the old standard, it is the one CMS I knew from before, and I thought I needed a fully fledged CMS, so I blindly ran towards it. Turns out, it has grown a lot since I remembered, it is now a fully fledged platform for complex websites and ecommerce development, even so I decided to give it a try, I picked a template and created a site. Even with the most simplistic and basic template I could find, there is a lot going on in the site. Setting it up was not as difficult or cumbersome as others claim, it took me about one hour to have it up and running, it looks good, but a bit crowded for my personal taste, and I found out it serves ads in your site for the readers, that is a big no for me.I have tried wix and squarespace before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.I discovered ghost as the platform used by one of the bloggers I follow (Sebastian Ruder), turns out is a fantastic evolution over wordpress. It runs on the latest technologies, its interface is quite modern, and it is focused on one thing only: publishing. They have a paid hosting service, but the software is open sourced, therefore free to use in any hosting.I also tested webflow and even created a mockup there, the learning curve was quite smooth, and its CMS seems quite robust, but a bit too much for the functionalities I required.Next were the generators that don’t have a web interface, but can be easily set up:The first I tried was netlify, I also set up a test site in it. Netlify provides free hosting, and to keep your source files it uses GitHub (a repository keeps the source files where it publishes from). It has its own CMS, Netlify CMS, and you have a choice of site generators: Hugo, Gatsby, MiddleMan, Preact CLI, Next.js, Elevently and Nuxt.js, and once you choose there are some templates for each. I did not find the variety of templates enticing enough, and the set up process was much more cumbersome than with wordpress (at least for my knowledge level). I choose Hugo for my test site.I also tested gatsby with it’s own Gatsby Cloud hosting service, here is my test site. They also use GitHub as a base to host the source files to build the website, so you create a repository, and it is connected to it. I found the free template offerings quite limited for what I was looking for.Finally it came the turn for jekyll, although an older, and slower generator (compared to Hugo and Gatsby), it was created by one of the founders of GitHub, so it’s integration with GitHub Pages is quite natural and painless, so much so, that to use them together you don’t even have to install Jekyll in your machine! You have two choices: keep it all online, by having one repository in Github keep all the source files, modify or add them online, and having Jekyll build and publish your site to the special gh-pages repository everytime you change or add a new file to the source repository. Have a synchronized local copy of the source files for the website, this way you can edit your blog and customize it in your choice of IDE (Integrated Development Environment). Then, when you update any file on your computer, you just “push” the changes to GitHub, and GitHub Pages automatically uses Jekyll to build and publish your site.I chose the second option, specially because I can manipulate files, like images, in my laptop, and everytime I sync my local repository with GitHub, they are updated and published automatically. Quite convenient.After testing with several templates to get the feel for it, I decided to keep Jekyll for my blog for several reasons: the convenience of not having to install anything extra on my computer to build my blog, the integration with GitHub Pages, the ease of use, the future proofing via integration with modern technologies such as react or vue and the vast online community that has produced tons of templates and useful information for issue resolution, customization and added functionality.I picked up a template, just forked the repository and started modifying the files to customize it, it was fast and easy, I even took it upon myself to add some functionality to the template (it served as a coding little project) like: SEO meta tags Dark mode (configurable in _config.yml file) automatic sitemap.xml automatic archive page with infinite scrolling capability new page of posts filtered by a single tag (without needing autopages from paginator V2), also with infinite scrolling click to tweet functionality (just add a &lt;tweet&gt; &lt;/tweet&gt; tag in your markdown. custom and responsive 404 page responsive and automatic Table of Contents (optional per post) read time per post automatically calculated responsive post tags and social share icons (sticky or inline) included linkedin, reddit and bandcamp icons copy link to clipboard sharing option (and icon) view on github link button (optional per post) MathJax support (optional per post) tag cloud in the home page ‘back to top’ button comments ‘courtain’ to mask the disqus interface until the user clicks on it (configurable in _config.yml) CSS variables to make it easy to customize all colors and fonts added several pygments themes for code syntax highlight configurable from the _config.yml file. See the highlighter directory for reference on the options. responsive footer menu and footer logo (if setup in the config file) smoother menu animationsAs a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites, but their complexity I think makes them useful for a big site with plenty of posts. For a small site like mine, Jekyll provides sufficient functionality and power without the hassle.You can use the modified template yourself by forking my repository. Let me know in the comments or feel free to contact me if you are interested in a detailed walkthrough on how to set it all up.HostingSince I decided on Jekyll to generate my site, the choice for hosting was quite obvious, Github Pages is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (the-mvm.github.io).Interplanetary File SystemTo contribute to and test IPFS I also set up a mirror in IPFS by using fleek.co. I must confess that it was more troublesome than I imagined, it was definetively not plug and play because of the paths used to fetch resources. The nature of IPFS makes short absolute paths for website resources (like images, css and javascript files) inoperative; the easiest fix for this is to use relative paths, however the same relative path that works for the root directory (i.e. /index.html) does not work for links inside directories (i.e. /tags/), and since the site is static, while generating it, one must make the distinction between the different directory levels for the page to be rendered correctly.At first I tried a simple (but brute force solution):# determine the level of the current file{% assign lvl = page.url | append:'X' | split:'/' | size %}# create the relative base (i.e. \"../\"){% capture relativebase %}{% for i in (3..lvl) %}../{% endfor %}{% endcapture %}{% if relativebase == '' %} {% assign relativebase = './' %}{% endif %}...# Eliminate unecesary double backslashes{% capture post_url %}{{ relativebase }}{{ post.url }}{% endcapture %}{% assign post_url = post_url | replace: \"//\", \"/\" %}This jekyll/liquid code was executed in every page (or include) that needed to reference a resource hosted in the same server.But this fix did not work for the search function, because it relies on a search.json file (also generated programmatically to be served as a static file), therefore when generating this file one either use the relative path for the root directory or for a nested directory, thus the search results will only link correctly the corresponding pages if the page where the user searched for something is in the corresponding scope.So the final solution was to make the whole site flat, meaning to live in a single directory. All pages and posts will live under the root directory, and by doing so, I can control how to address the relative paths for resources."
    } ,
  
    {
      "title"       : "Deep Q Learning for Tic Tac Toe",
      "category"    : "",
      "tags"        : "machine learning, artificial intelligence, reinforcement learning, coding, python",
      "url"         : "./deep-q-learning-tic-tac-toe.html",
      "date"        : "2021-03-19 05:14:20 +0800",
      "description" : "Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?",
      "content"     : "BackgroundAfter many years of a corporate career (17) diverging from computer science, I have now decided to learn Machine Learning and in the process return to coding (something I have always loved!).To fully grasp the essence of ML I decided to start by coding a ML library myself, so I can fully understand the inner workings, linear algebra and calculus involved in Stochastic Gradient Descent. And on top learn Python (I used to code in C++ 20 years ago).I built a general purpose basic ML library that creates a Neural Network (only DENSE layers), saves and loads the weights into a file, does forward propagation and training (optimization of weights and biases) using SGD. I tested the ML library with the XOR problem to make sure it worked fine. You can read the blog post for it here.For the next challenge I am interested in reinforcement learning greatly inspired by Deep Mind’s astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe (or noughts and crosses).How hard could it be?Of course the first thing to do was to program the game itself, so I chose Python because I am learning it, so it gives me a good practice opportunity, and PyGame for the interface.Coding the game was quite straightforward, albeit for the hiccups of being my first PyGame and almost my first Python program ever.I created the game quite openly, in such a way that it can be played by two humans, by a human vs. an algorithmic AI, and a human vs. the neural network. And of course the neural network against a choice of 3 AI engines: random, minimax or hardcoded (an exercise I wanted to do since a long time).While training, the visuals of the game can be disabled to make training much faster.Now, for the fun part, training the network, I followed Deep Mind’s own DQN recommendations:The network will be an approximation for the Q value function or Bellman equation, meaning that the network will be trained to predict the \"value\" of each move available in a given game state.A replay experience memory was implemented. This meant that the neural network will not be trained after each move. Each move will be recorded in a special \"memory\" alongside with the state of the board and the reward it received for taking such an action (move).After the memory is sizable enough, batches of random experiences sampled from the replay memory are used for every training roundA secondary neural network (identical to the main one) is used to calculate part of the Q value function (Bellman equation), in particular the future Q values. And then it is updated with the main network's weights every n games. This is done so that we are not chasing a moving target.Designing the neural networkThe Neural Network chosen takes 9 inputs (the current state of the game) and outputs 9 Q values for each of the 9 squares in the board of the game (possible actions). Obviously some squares are illegal moves, hence while training there was a negative reward given to illegal moves hoping that the model would learn not to play illegal moves in a given position.I started out with two hidden layers of 36 neurons each, all fully connected and activated via ReLu. The output layer was initially activated using sigmoid to ensure that we get a nice value between 0 and 1 that represents the QValue of a given state action pair.The many models…Model 1 - the first tryAt first the model was trained by playing vs. a “perfect” AI, meaning a hard coded algorithm that never looses and that will win if it is given the chance. After several thousand training rounds, I noticed that the Neural Network was not learning much; so I switched to training vs. a completely random player, so that it will also learn how to win. After training vs. the random player, the Neural Network seems to have made progress and is steadily diminishing the loss function over time.However, the model was still generating many illegal moves, so I decided to modify the reinforcement learning algorithm to punish more the illegal moves. The change consisted in populating with zeros all the corresponding illegal moves for a given position at the target values to train the network. This seemed to work very well for diminishing the illegal moves:Nevertheless, the model was still performing quite poorly winning only around 50% of games vs. a completely random player (I expected it to win above 90% of the time). This was after only training 100,000 games, so I decided to keep training and see the results:Wins: 65.46% Losses: 30.32% Ties: 4.23%Note that when training restarts, the loss and illegal moves are still high in the beginning of the training round, and this is caused by the epsilon greedy strategy that prefers exploration (a completely random move) over exploitation, this preference diminishes over time.After another round of 100,000 games, I can see that the loss function actually started to diminish, and the win rate ended up at 65%, so with little hope I decided to carry on and do another round of 100,000 games (about 2 hours in an i7 MacBook Pro):Wins: 46.40% Losses: 41.33% Ties: 12.27%As you can see in the chart, the calculated loss not even plateaued, but it seemed to increase a bit over time, which tells me the model is not learning anymore. This was confirmed by the win rate decreasing with respect of the previous round to a meek 46.4% that looks no better than a random player.Model 2 - Linear activation for the outputAfter not getting the results I wanted, I decided to change the output activation function to linear, since the output is supposed to be a Q value, and not a probability of an action.Wins: 47.60% Losses: 39% Ties: 13.4%Initially I tested with only 1000 games to see if the new activation function was working, the loss function appears to be decreasing, however it reached a plateau around a value of 1, hence still not learning as expected. I came across a technique by Brad Kenstler, Carl Thome and Jeremy Jordan called Cyclical Learning Rate, which appears to solve some cases of stagnating loss functions in this type of networks. So I gave it a go using their Triangle 1 model.With the cycling learning rate in place, still no luck after a quick 1,000 games training round; so I decided to implement on top a decaying learning rate as per the following formula:The resulting learning rate combining the cycles and decay per epoch is:Learning Rate = 0.1, Decay = 0.0001, Cycle = 2048 epochs, max Learning Rate factor = 10xtrue_epoch = epoch - c.BATCH_SIZElearning_rate = self.learning_rate*(1/(1+c.DECAY_RATE*true_epoch))if c.CLR_ON: learning_rate = self.cyclic_learning_rate(learning_rate,true_epoch)@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate*c.MAX_LR_FACTOR cycle = np.floor(1+(epoch/(2*c.LR_STEP_SIZE))) x = np.abs((epoch/c.LR_STEP_SIZE)-(2*cycle)+1) return learning_rate+(max_lr-learning_rate)*np.maximum(0,(1-x))c.DECAY_RATE = learning rate decay ratec.MAX_LR_FACTOR = multiplier that determines the max learning ratec.LR_STEP_SIZE = the number of epochs each cycle lastsWith these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.1,000,000 episodes, 7.5 million epochs with batches of 64 moves eachWins: 52.66% Losses: 36.02% Ties: 11.32%After 24 hours!, my computer was able to run 1,000,000 episodes (games played), which represented 7.5 million training epochs of batches of 64 plays (480 million plays learned), the learning rate did decreased (a bit), but is clearly still in a plateau; interestingly, the lower boundary of the loss function plot seems to continue to decrease as the upper bound and the moving average remains constant. This led me to believe that I might have hit a local minimum.Model 3 - new network topologyAfter all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.100,000 episodes, 635,000 epochs with batches of 64 moves eachWins: 76.83% Losses: 17.35% Ties: 5.82%I increased to 200 neurons each hidden layer. In spite of this great improvement the loss function was still in a plateau at around 0.1 (Mean Squared Error). Which, although it is greatly reduced from what we had, still was giving out only 77% win rate vs. a random player, the network was playing tic tac toe as a toddler!*I can still beat the network most of the time! (I am playing with the red X)*100,000 more episodes, 620,000 epochs with batches of 64 moves eachWins: 82.25% Losses: 13.28% Ties: 4.46%Finally we crossed the 80% mark! This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.After more training rounds and some experimenting with the learning rate and other parameters, I couldn’t improve past the 82.25% win rate.These have been the results so far:It is quite interesting to learn how the many parameters (hyper-parameters as most authors call them) of a neural network model affect its training performance, I have played with: the learning rate the network topology and activation functions the cycling and decaying learning rate parameters the batch size the target update cycle (when the target network is updated with the weights from the policy network) the rewards policy the epsilon greedy strategy whether to train vs. a random player or an “intelligent” AI.And so far the most effective change has been the network topology, but being so close but not quite there yet to my goal of 90% win rate vs. a random player, I will still try to optimize further.Network topology seems to have the biggest impact on a neural network's learning ability.Model 4 - implementing momentumI reached out to the reddit community and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm. So I did some research and ended up deciding to implement various optimization methods to experiment with: Stochastic Gradient Descent with Momentum RMSProp: Root Mean Square Plain Momentum NAG: Nezterov’s Accelerated Momentum Adam: Adaptive Moment Estimation and keep my old vanilla Gradient Descent (vGD) ☺Click here for a detailed explanation and code of all the implemented optimization algorithms.So far, I have not been able to get better results with Model 4, I have tried all the momentum optimization algorithms with little to no success.Model 5 - implementing one-hot encoding and changing topology (again)I came across an interesting project in Github that deals exactly with Deep Q Learning, and I noticed that he used “one-hot” encoding for the input as opposed to directly entering the values of the player into the 9 input slots. So I decided to give it a try and at the same time change my topology to match his:So, ‘one hot’ encoding is basically changing the input of a single square in the tic tac toe board to three numbers, so that each state is represented with different inputs, thus the network can clearly differentiate the three of them. As the original author puts it, the way I was encoding, having 0 for empty, 1 for X and 2 for O, the network couldn’t easily tell that, for instance, O and X both meant occupied states, because one is two times as far from 0 as the other. With the new encoding, the empty state will be 3 inputs: (1,0,0), the X will be (0,1,0) and the O (0,0,1) as in the diagram.Still, no luck even with Model 5, so I am starting to think that there could be a bug in my code.To test this hypothesis, I decided to implement the same model using Tensorflow / Keras.Model 6 - Tensorflow / Kerasself.PolicyNetwork = Sequential()for layer in hidden_layers: self.PolicyNetwork.add(Dense( units=layer, activation='relu', input_dim=inputs, kernel_initializer='random_uniform', bias_initializer='zeros'))self.PolicyNetwork.add(Dense( outputs, kernel_initializer='random_uniform', bias_initializer='zeros'))opt = Adam(learning_rate=c.LEARNING_RATE, beta_1=c.GAMMA_OPT, beta_2=c.BETA, epsilon=c.EPSILON, amsgrad=False)self.PolicyNetwork.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.The training function changed to:reduce_lr_on_plateau = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=25)history = self.PolicyNetwork.fit(np.asarray(states_to_train), np.asarray(targets_to_train), epochs=c.EPOCHS, batch_size=c.BATCH_SIZE, verbose=1, callbacks=[reduce_lr_on_plateau], shuffle=True)With Tensorflow implemented, the first thing I noticed, was that I had an error in the calculation of the loss, although this only affected reporting and didn’t change a thing on the training of the network, so the results kept being the same, the loss function was still stagnating! My code was not the issue.Model 7 - changing the training scheduleNext I tried to change the way the network was training as per u/elBarto015 advised me on reddit.The way I was training initially was: Games begin being simulated and the outcome recorded in the replay memory Once a sufficient ammount of experiences are recorded (at least equal to the batch size) the Network will train with a random sample of experiences from the replay memory. The ammount of experiences to sample is the batch size. The games continue to be played between the random player and the network. Every move from either player generates a new training round, again with a random sample from the replay memory. This continues until the number of games set up conclude.The first change was to train only after every game concludes with the same ammount of data (a batch). This was still not giving any good results.The second change was more drastic, it introduced the concept of epochs for every training round, it basically sampled the replay memory for epochs * batch size experiences, for instance if epochs selected were 10, and batch size was 81, then 810 experiences were sampled out of the replay memory. With this sample the network was then trained for 10 epochs randomly using the batch size.This meant that I was training now effectively 10 (or the number of epochs selected) times more per game, but in batches of the same size and randomly shuffling the experiences each epoch.After still playing around with some hyperparameters I managed to get similar performance as I got before, reaching 83.15% win rate vs. the random player, so I decided to keep training in rounds of 2,000 games each to evaluate performance. With almost every round I could see improvement:As of today, my best result so far is 87.5%, I will leave it rest for a while and keep investigating to find a reason for not being able to reach at least 90%. I read about self play, and it looks like a viable option to test and a fun coding challenge. However, before embarking in yet another big change I want to ensure I have been thorough with the model and have tested every option correctly.I feel the end is near… should I continue to update this post as new events unfold or shall I make it a multi post thread?"
    } ,
  
    {
      "title"       : "Neural Network Optimization Methods and Algorithms",
      "category"    : "",
      "tags"        : "coding, machine learning, optimization, deep Neural networks",
      "url"         : "./neural-network-optimization-methods.html",
      "date"        : "2021-03-13 03:32:20 +0800",
      "description" : "Some neural network optimization algorithms mostly to implement momentum when doing back propagation.",
      "content"     : "For the seemingly small project I undertook of creating a machine learning neural network that could learn by itself to play tic-tac-toe, I bumped into the necesity of implementing at least one momentum algorithm for the optimization of the network during backpropagation.And since my original post for the TicTacToe project is quite large already, I decided to post separately these optimization methods and how did I implement them in my code.AdamsourceAdaptive Moment Estimation (Adam) is an optimization method that computes adaptive learning rates for each weight and bias. In addition to storing an exponentially decaying average of past squared gradients \\(v_t\\) and an exponentially decaying average of past gradients \\(m_t\\), similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients \\(m_t\\) and \\(v_t\\) respectively as follows:\\(\\begin{align}\\begin{split}m_t &amp;= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\v_t &amp;= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\end{split}\\end{align}\\)\\(m_t\\) and \\(v_t\\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \\(m_t\\) and \\(v_t\\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \\(\\beta_1\\) and \\(\\beta_2\\) are close to 1).They counteract these biases by computing bias-corrected first and second moment estimates:\\(\\begin{align}\\begin{split}\\hat{m}_t &amp;= \\dfrac{m_t}{1 - \\beta^t_1} \\\\\\hat{v}_t &amp;= \\dfrac{v_t}{1 - \\beta^t_2} \\end{split}\\end{align}\\)We then use these to update the weights and biases which yields the Adam update rule:\\(\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\\).The authors propose defaults of 0.9 for \\(\\beta_1\\), 0.999 for \\(\\beta_2\\), and \\(10^{-8}\\) for \\(\\epsilon\\).view on github# decaying averages of past gradientsself.v[\"dW\" + str(i)] = ((c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) ))self.v[\"db\" + str(i)] = ((c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) ))# decaying averages of past squared gradientsself.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square(np.array( self.bias_gradients[i]))) ))if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch))# apply to weights and biasesweight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON))))self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON))))SGD MomentumsourceVanilla SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction \\(\\gamma\\) of the update vector of the past time step to the current update vector:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)The momentum term \\(\\beta_1\\) is usually set to 0.9 or a similar value.Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \\(\\beta_1 &lt; 1\\)). The same thing happens to our weight and biases updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.view on githubself.v[\"dW\"+str(i)] = ((c.BETA1*self.v[\"dW\" + str(i)]) +(eta*np.array(self.gradients[i]) ))self.v[\"db\"+str(i)] = ((c.BETA1*self.v[\"db\" + str(i)]) +(eta*np.array(self.bias_gradients[i]) ))weight_col -= self.v[\"dW\" + str(i)]self.bias[i] -= self.v[\"db\" + str(i)]Nesterov accelerated gradient (NAG)sourceHowever, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \\(\\beta_1 v_{t-1}\\) to move the weights and biases \\(\\theta\\). Computing \\( \\theta - \\beta_1 v_{t-1} \\) thus gives us an approximation of the next position of the weights and biases (the gradient is missing for the full update), a rough idea where our weights and biases are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current weights and biases \\(\\theta\\) but w.r.t. the approximate future position of our weights and biases:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta - \\beta_1 v_{t-1} ) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)Again, we set the momentum term \\(\\beta_1\\) to a value of around 0.9. While Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update. This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of Neural Networks on a number of tasks.Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual weight and bias to perform larger or smaller updates depending on their importance.view on githubv_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]}self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i]))self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i]))weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)])self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)])RMSpropsourceRMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.RMSprop was developed stemming from the need to resolve other method's radically diminishing learning rates.\\(\\begin{align}\\begin{split}E[\\theta^2]_t &amp;= \\beta_1 E[\\theta^2]_{t-1} + (1-\\beta_1) \\theta^2_t \\\\\\theta_{t+1} &amp;= \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[\\theta^2]_t + \\epsilon}} \\theta_{t}\\end{split}\\end{align}\\)RMSprop divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \\(\\beta_1\\) to be set to 0.9, while a good default value for the learning rate \\(\\eta\\) is 0.001.view on githubself.s[\"dW\" + str(i)] = ((c.BETA1 * self.s[\"dW\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA1 * self.s[\"db\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.bias_gradients[i]))) ))weight_col -= (eta * (np.array(self.gradients[i]) / (np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) )self.bias[i] -= (eta * (np.array(self.bias_gradients[i]) / (np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) )Complete codeAll in all the code ended up like this:view on github@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate * c.MAX_LR_FACTOR cycle = np.floor(1 + (epoch / (2 * c.LR_STEP_SIZE)) ) x = np.abs((epoch / c.LR_STEP_SIZE) - (2 * cycle) + 1) return learning_rate + (max_lr - learning_rate) * np.maximum(0, (1 - x))def apply_gradients(self, epoch): true_epoch = epoch - c.BATCH_SIZE eta = self.learning_rate * (1 / (1 + c.DECAY_RATE * true_epoch)) if c.CLR_ON: eta = self.cyclic_learning_rate(eta, true_epoch) for i, weight_col in enumerate(self.weights): if c.OPTIMIZATION == 'vanilla': weight_col -= eta * np.array(self.gradients[i]) / c.BATCH_SIZE self.bias[i] -= eta * np.array(self.bias_gradients[i]) / c.BATCH_SIZE elif c.OPTIMIZATION == 'SGD_momentum': self.v[\"dW\"+str(i)] = ((c.BETA1 *self.v[\"dW\" + str(i)]) +(eta *np.array(self.gradients[i]) )) self.v[\"db\"+str(i)] = ((c.BETA1 *self.v[\"db\" + str(i)]) +(eta *np.array(self.bias_gradients[i]) )) weight_col -= self.v[\"dW\" + str(i)] self.bias[i] -= self.v[\"db\" + str(i)] elif c.OPTIMIZATION == 'NAG': v_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]} self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i])) self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i])) weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)]) self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)]) elif c.OPTIMIZATION == 'RMSProp': self.s[\"dW\" + str(i)] = ((c.BETA1 *self.s[\"dW\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA1 *self.s[\"db\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.bias_gradients[i]))) )) weight_col -= (eta *(np.array(self.gradients[i]) /(np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) ) self.bias[i] -= (eta *(np.array(self.bias_gradients[i]) /(np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) ) if c.OPTIMIZATION == \"ADAM\": # decaying averages of past gradients self.v[\"dW\" + str(i)] = (( c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) )) self.v[\"db\" + str(i)] = (( c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) )) # decaying averages of past squared gradients self.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.bias_gradients[i]))) )) if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) # apply to weights and biases weight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON)))) self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON)))) self.gradient_zeros()"
    } ,
  
    {
      "title"       : "Machine Learning Library in Python from scratch",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks, python",
      "url"         : "./ML-Library-from-scratch.html",
      "date"        : "2021-03-01 02:32:20 +0800",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "It must sound crazy that in this day and age, when we have such a myriad of amazing machine learning libraries and toolkits all open sourced, all quite well documented and easy to use, I decided to create my own ML library from scratch.Let me try to explain; I am in the process of immersing myself into the world of Machine Learning, and to do so, I want to deeply understand the basic concepts and its foundations, and I think that there is no better way to do so than by creating myself all the code for a basic neural network library from scratch. This way I can gain in depth understanding of the math that underpins the ML algorithms.Another benefit of doing this is that since I am also learning Python, the experiment brings along good exercise for me.To call it a Machine Learning Library is perhaps a bit of a stretch, since I just intended to create a multi-neuron, multi-layered perceptron.The library started very narrowly, with just the following functionality: create a neural network based on the following parameters: number of inputs size and number of hidden layers number of outputs learning rate forward propagate or predict the output values when given some inputs learn through back propagation using gradient descentI restricted the model to be sequential, and the layers to be only dense / fully connected, this means that every neuron is connected to every neuron of the following layer. Also, as a restriction, the only activation function I implemented was sigmoid:With my neural network coded, I tested it with a very basic problem, the famous XOR problem.XOR is a logical operation that cannot be solved by a single perceptron because of its linearity restriction:As you can see, when plotted in an X,Y plane, the logical operators AND and OR have a line that can clearly separate the points that are false from the ones that are true, hence a perceptron can easily learn to classify them; however, for XOR there is no single straight line that can do so, therefore a multilayer perceptron is needed for the task.For the test I created a neural network with my library:import Neural_Network as nninputs = 3hidden_layers = [2, 1]outputs = 1learning_rate = 0.03NN = nn.NeuralNetwork(inputs, hidden_layers, outputs, learning_rate)The three inputs I decided to use (after a lot of trial and error) are the X and Y coordinate of a point (between X = 0, X = 1, Y = 0 and Y = 1) and as the third input the multiplication of both X and Y. Apparently it gives the network more information, and it ends up converging much more quickly with this third input.Then there is a single hidden layer with 2 neurons and one output value, that will represent False if the value is closer to 0 or True if the value is closer to 1.Then I created the learning data, which is quite trivial for this problem, since we know very easily how to compute XOR.training_data = []for n in range(learning_rounds): x = rnd.random() y = rnd.random() training_data.append([x, y, x * y, 0 if (x &lt; 0.5 and y &lt; 0.5) or (x &gt;= 0.5 and y &gt;= 0.5) else 1])And off we go into training:for data in training_data: NN.train(data[:3].reshape(inputs), data[3:].reshape(outputs))The ML library can only train on batches of 1 (another self-imposed coding restriction), therefore only one “observation” at a time, this is why the train function accepts two parameters, one is the inputs packed in an array, and the other one is the outputs, packed as well in an array.To see the neural net in action I decided to plot the predicted results in both a 3d X,Y,Z surface plot (z being the network’s predicted value), and a scatter plot with the color of the points representing the predicted value.This was plotted in MatPlotLib, so we needed to do some housekeeping first:fig = plt.figure()fig.canvas.set_window_title('Learning XOR Algorithm')fig.set_size_inches(11, 6)axs1 = fig.add_subplot(1, 2, 1, projection='3d')axs2 = fig.add_subplot(1, 2, 2)Then we need to prepare the data to be plotted by generating X and Y values distributed between 0 and 1, and having the network calculate the Z value:x = np.linspace(0, 1, num_surface_points)y = np.linspace(0, 1, num_surface_points)x, y = np.meshgrid(x, y)z = np.array(NN.forward_propagation([x, y, x * y])).reshape(num_surface_points, num_surface_points)As you can see, the z values array is reshaped as a 2d array of shape (x,y), since this is the way Matplotlib interprets it as a surface:axs1.plot_surface(x, y, z, rstride=1, cstride=1, cmap='viridis', vmin=0, vmax=1, antialiased=True)The end result looks something like this:Then we reshape the z array as a one dimensional array to use it to color the scatter plot:z = z.reshape(num_surface_points ** 2)scatter = axs2.scatter(x, y, marker='o', s=40, c=z.astype(float), cmap='viridis', vmin=0, vmax=1)To actually see the progress while learning, I created a Matplotlib animation, and it is quite interesting to see as it learns. So my baby ML library is completed for now, but still I would like to enhance it in several ways: include multiple activation functions (ReLu, linear, Tanh, etc.) allow for multiple optimizers (Adam, RMSProp, SGD Momentum, etc.) have batch and epoch training schedules functionality save and load trained model to fileI will get to it soon…"
    } ,
  
    {
      "title"       : "Conway&#39;s Game of Life",
      "category"    : "",
      "tags"        : "coding, python",
      "url"         : "./conways-game-of-life.html",
      "date"        : "2021-02-11 03:32:20 +0800",
      "description" : "Taking on the challenge of picking up coding again through interesting small projects, this time it is the turn of Conway's Game of Life.",
      "content"     : "I&nbsp;am lately trying to take on coding again. It had always been a part of my life since my early years when I&nbsp;learned to program a Tandy Color Computer at the age of 8, the good old days.Tandy Color Computer TRS80 IIIHaving already programed in Java, C# and of course BASIC, I&nbsp;thought it would be a great idea to learn Python since I&nbsp;have great interest in data science and machine learning, and those two topics seem to have an avid community within Python coders.For one of my starter quick programming tasks, I&nbsp;decided to code Conway's Game of Life, a very simple cellular automata that basically plays itself.The game consists of a grid of n size, and within each block of the grid a cell could either be dead or alive according to these rules:If a cell has less than 2 neighbors, meaning contiguous alive cells, the cell will die of lonelinessIf a cell has more than 3 neighbors, it will die of overpopulationIf an empty block has exactly 3 contiguous alive neighbors, a new cell will be born in that spotIf an alive cell has 2 or 3 alive neighbors, it continues to liveConway’s rules for the Game of LifeTo make it more of a challenge I&nbsp;also decided to implement an \"sparse\" method of recording the game board, this means that instead of the typical 2d array representing the whole board, I&nbsp;will only record the cells which are alive. Saving a lot of memory space and processing time, while adding some spice to the challenge.The trickiest part was figuring out how to calculate which empty blocks had exactly 3 alive neighbors so that a new cell will spring to life there, this is trivial in the case of recording the whole grid, because we just iterate all over the board and find the alive neighbors of ALL&nbsp;the blocks in the grid, but in the case of only keeping the alive cells proved quite a challenge.In the end the algorithm ended up as follows:Iterate through all the alive cells and get all of their neighborsdef get_neighbors(self, cell): neighbors = [] for x in range(-1, 2, 1): for y in range(-1, 2, 1): if not (x == 0 and y == 0): if (0 &amp;lt;= (cell[0] + x) &amp;lt;= self.size_x) and (0 &amp;lt;= (cell[1] + y) &amp;lt;= self.size_y): neighbors.append((cell[0] + x, cell[1] + y)) return neighborsMark all the neighboring blocks as having +1 neighbor each time a particular cell is encountered. This way, for each neighboring alive cell the counter of the particular block will increase, and in the end it will contain the total number of live cells which are contiguous to it.def next_state(self): alive_neighbors = {} for cell in self.alive_cells: if cell not in alive_neighbors: alive_neighbors[cell] = 0 neighbors = self.get_neighbors(cell) for neighbor in neighbors: if neighbor not in alive_neighbors: alive_neighbors[neighbor] = 1 else: alive_neighbors[neighbor] += 1The trick was using a dictionary to keep the record of the blocks that have alive neighbors and the cells who are alive in the current state but have zero alive neighbors (thus will die).With the dictionary it became easy just to add cells and increase their neighbor counter each time it was encountered as a neighbor of an alive cell.Having the dictionary now filled with all the cells that have alive neighbors and how many they have, it was just a matter of applying the rules of the game:for cell in alive_neighbors: if alive_neighbors[cell] &amp;lt; 2 or alive_neighbors[cell] &gt; 3: self.alive_cells.discard(cell) elif alive_neighbors[cell] == 3: self.alive_cells.add(cell)Notice that since I am keeping an array of the coordinates of only the cells who are alive, I could apply just 3 rules, die of loneliness, die of overpopulation and become alive from reproduction (exactly 3 alive neighbors) because the ones who have 2 or 3 neighbors and are already alive, can remain alive in the next iteration.I&nbsp;found it very interesting to implement the Game of Life like this, it was quite a refreshing challenge and I am beginning to feel my coding skills ramping up again."
    } ,
  
    {
      "title"       : "Single Neuron Perceptron",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks",
      "url"         : "./single-neuron-perceptron.html",
      "date"        : "2021-01-26 03:32:20 +0800",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.What is a perceptron?A perceptron is the basic building block of a neural network, it can be compared to a neuron, And its conception is what detonated the vast field of Artificial Intelligence nowadays.Back in the late 1950’s, a young Frank Rosenblatt devised a very simple algorithm as a foundation to construct a machine that could learn to perform different tasks.In its essence, a perceptron is nothing more than a collection of values and rules for passing information through them, but in its simplicity lies its power.Imagine you have a ‘neuron’ and to ‘activate’ it, you pass through several input signals, each signal connects to the neuron through a synapse, once the signal is aggregated in the perceptron, it is then passed on to one or as many outputs as defined. A perceptron is but a neuron and its collection of synapses to get a signal into it and to modify a signal to pass on.In more mathematical terms, a perceptron is an array of values (let’s call them weights), and the rules to apply such values to an input signal.For instance a perceptron could get 3 different inputs as in the image, lets pretend that the inputs it receives as signal are: $x_1 = 1, \\; x_2 = 2\\; and \\; x_3 = 3$, if it’s weights are $w_1 = 0.5,\\; w_2 = 1\\; and \\; w_3 = -1$ respectively, then what the perceptron will do when the signal is received is to multiply each input value by its corresponding weight, then add them up.\\(\\begin{align}\\begin{split}\\left(x_1 * w_1\\right) + \\left(x_2 * w_2\\right) + \\left(x_3 * w_3\\right)\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\left(0.5 * 1\\right) + \\left(1 * 2\\right) + \\left(-1 * 3\\right) = 0.5 + 2 - 3 = -0.5\\end{split}\\end{align}\\)Typically when this value is obtained, we need to apply an “activation” function to smooth the output, but let’s say that our activation function is linear, meaning that we keep the value as it is, then that’s it, that is the output of the perceptron, -0.5.In a practical application, the output means something, perhaps we want our perceptron to classify a set of data and if the perceptron outputs a negative number, then we know the data is of type A, and if it is a positive number then it is of type B.Once we understand this, the magic starts to happen through a process called backpropagation, where we “educate” our tiny one neuron brain to have it learn how to do its job.The magic starts to happen through a process called backpropagation, where we \"educate\" our tiny one neuron brain to have it learn how to do its job.For this we need a set of data that it is already classified, we call this a training set. This data has inputs and their corresponding correct output. So we can tell the little brain when it misses in its prediction, and by doing so, we also adjust the weights a bit in the direction where we know the perceptron committed the mistake hoping that after many iterations like this the weights will be so that most of the predictions will be correct.After the model trains successfully we can have it classify data it has never seen before, and we have a fairly high confidence that it will do so correctly.The math behind this magical property of the perceptron is called gradient descent, and is just a bit of differential calculus that helps us convert the error the brain is having into tiny nudges of value of the weights towards their optimum. This video series by 3 blue 1 brown explains it wonderfuly.My program creates a single neuron neural network tuned to guess if a point is above or below a randomly generated line and generates a visualization based on graphs to see how the neural network is learning through time.The neuron has 3 inputs and weights to calculate its output:input 1 is the X coordinate of the point,Input 2 is the y coordinate of the point,Input 3 is the bias and it is always 1Input 3 or the bias is required for lines that do not cross the origin (0,0)The Perceptron starts with weights all set to zero and learns by using 1,000 random points per each iteration.The output of the perceptron is calculated with the following activation function: if x * weight_x + y weight_y + weight_bias is positive then 1 else 0The error for each point is calculated as the expected outcome of the perceptron minus the real outcome therefore there are only 3 possible error values: Expected Calculated Error 1 -1 1 1 1 0 -1 -1 0 -1 1 -1 With every point that is learned if the error is not 0 the weights are adjusted according to:New_weight = Old_weight + error * input * learning_ratefor example: New_weight_x = Old_weight_x + error * x * learning rateA very useful parameter in all of neural networks is teh learning rate, which is basically a measure on how tiny our nudge to the weights is going to be.In this particular case, I coded the learning_rate to decrease with every iteration as follows:learning_rate = 0.01 / (iteration + 1)this is important to ensure that once the weights are nearing the optimal values the adjustment in each iteration is subsequently more subtle.In the end, the perceptron always converges into a solution and finds with great precision the line we are looking for.Perceptrons are quite a revelation in that they can resolve equations by learning, however they are very limited. By their nature they can only resolve linear equations, so their problem space is quite narrow.Nowadays the neural networks consist of combinations of many perceptrons, in many layers, and other types of “neurons”, like convolution, recurrent, etc. increasing significantly the types of problems they solve."
    } 
  
]
